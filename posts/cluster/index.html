<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>

<title>KNN&amp;DBscan&amp;K-means实现 | Asiv&#39;s Blog</title>
<meta name="keywords" content="Machine Learning, KNN, DBscan, K-means">
<meta name="description" content="Python集成开发环境(IDE)
(1) Anaconda: https://www.continuum.io/ （推荐）
(2) IDLE: Python解释器默认工具
(3) PyCharm: https://www.jetbrains.com/pycharm/
(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。
需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证
鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。
数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是cm，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：
属性 最大值 最小值 均值 方差 萼长 7.9 4.3 5.84 0.83 萼宽 4.4 2.0 3.05 0.43 瓣长 6.9 1.0 3.76 1.76 瓣宽 2.5 0.1 1.20 0.76 KNN算法 算法原理 存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。
基本概念 计算距离 常用到的距离计算公式如下：
欧几里得距离（欧氏距离）：
$d=\sqrt{(x_{1}-x_{2})^{2}&#43;(y_{1}-y_{2})^{2}}$​
曼哈顿距离
闵可夫斯基距离
切比雪夫距离
马氏距离
余弦相似度
皮尔逊相关系数
汉明距离
寻找最近邻数据
将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。 关于K值得选择问题：
K 值的选择会对算法的结果产生重大影响。 K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。 如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。 在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差） 三种交叉验证方法：
Hold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 决策分类
明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：
多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。 说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。
​	1）计算待分类点与已知类别的点之间的距离
​	2）按照距离递增次序排序
​	3）选取与待分类点距离最小的K个点
​	4）确定前K个点所在类别的出现次数
​	5）返回前K个点出现次数最高的类别作为待分类点的预测分类
代码实现 初始化数据集 初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。
iris = load_iris() data = iris.data label = iris.target # 划分训练集和测试集 train_set, test_set, train_label, test_label = train_test_split(data, label, test_size=0.2) 数据处理 归一化处理
train_set = (train_set - train_set.min(*axis*=0)) / (train_set.max(*axis*=0) - train_set.min(*axis*=0)) test_set = (test_set - test_set.">
<meta name="author" content="Asiv">
<link rel="canonical" href="https://niceasiv.cn/posts/cluster/">
<meta name="google-site-verification" content="NiceAsiv.cn">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://niceasiv.cn/img/logo.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://niceasiv.cn/img/logo.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://niceasiv.cn/img/logo.jpg">
<link rel="apple-touch-icon" href="https://niceasiv.cn/img/logo.jpg">
<link rel="mask-icon" href="https://niceasiv.cn/img/logo.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<meta property="og:title" content="KNN&amp;DBscan&amp;K-means实现" />
<meta property="og:description" content="Python集成开发环境(IDE)
(1) Anaconda: https://www.continuum.io/ （推荐）
(2) IDLE: Python解释器默认工具
(3) PyCharm: https://www.jetbrains.com/pycharm/
(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。
需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证
鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。
数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是cm，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：
属性 最大值 最小值 均值 方差 萼长 7.9 4.3 5.84 0.83 萼宽 4.4 2.0 3.05 0.43 瓣长 6.9 1.0 3.76 1.76 瓣宽 2.5 0.1 1.20 0.76 KNN算法 算法原理 存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。
基本概念 计算距离 常用到的距离计算公式如下：
欧几里得距离（欧氏距离）：
$d=\sqrt{(x_{1}-x_{2})^{2}&#43;(y_{1}-y_{2})^{2}}$​
曼哈顿距离
闵可夫斯基距离
切比雪夫距离
马氏距离
余弦相似度
皮尔逊相关系数
汉明距离
寻找最近邻数据
将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。 关于K值得选择问题：
K 值的选择会对算法的结果产生重大影响。 K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。 如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。 在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差） 三种交叉验证方法：
Hold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 决策分类
明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：
多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。 说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。
​	1）计算待分类点与已知类别的点之间的距离
​	2）按照距离递增次序排序
​	3）选取与待分类点距离最小的K个点
​	4）确定前K个点所在类别的出现次数
​	5）返回前K个点出现次数最高的类别作为待分类点的预测分类
代码实现 初始化数据集 初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。
iris = load_iris() data = iris.data label = iris.target # 划分训练集和测试集 train_set, test_set, train_label, test_label = train_test_split(data, label, test_size=0.2) 数据处理 归一化处理
train_set = (train_set - train_set.min(*axis*=0)) / (train_set.max(*axis*=0) - train_set.min(*axis*=0)) test_set = (test_set - test_set." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niceasiv.cn/posts/cluster/" /><meta property="og:image" content="https://niceasiv.cn/images/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-11-22T20:51:09+00:00" />
<meta property="article:modified_time" content="2022-11-22T20:51:09+00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://niceasiv.cn/images/papermod-cover.png"/>

<meta name="twitter:title" content="KNN&amp;DBscan&amp;K-means实现"/>
<meta name="twitter:description" content="Python集成开发环境(IDE)
(1) Anaconda: https://www.continuum.io/ （推荐）
(2) IDLE: Python解释器默认工具
(3) PyCharm: https://www.jetbrains.com/pycharm/
(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。
需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证
鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。
数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是cm，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：
属性 最大值 最小值 均值 方差 萼长 7.9 4.3 5.84 0.83 萼宽 4.4 2.0 3.05 0.43 瓣长 6.9 1.0 3.76 1.76 瓣宽 2.5 0.1 1.20 0.76 KNN算法 算法原理 存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。
基本概念 计算距离 常用到的距离计算公式如下：
欧几里得距离（欧氏距离）：
$d=\sqrt{(x_{1}-x_{2})^{2}&#43;(y_{1}-y_{2})^{2}}$​
曼哈顿距离
闵可夫斯基距离
切比雪夫距离
马氏距离
余弦相似度
皮尔逊相关系数
汉明距离
寻找最近邻数据
将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。 关于K值得选择问题：
K 值的选择会对算法的结果产生重大影响。 K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。 如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。 在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差） 三种交叉验证方法：
Hold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 决策分类
明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：
多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。 说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。
​	1）计算待分类点与已知类别的点之间的距离
​	2）按照距离递增次序排序
​	3）选取与待分类点距离最小的K个点
​	4）确定前K个点所在类别的出现次数
​	5）返回前K个点出现次数最高的类别作为待分类点的预测分类
代码实现 初始化数据集 初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。
iris = load_iris() data = iris.data label = iris.target # 划分训练集和测试集 train_set, test_set, train_label, test_label = train_test_split(data, label, test_size=0.2) 数据处理 归一化处理
train_set = (train_set - train_set.min(*axis*=0)) / (train_set.max(*axis*=0) - train_set.min(*axis*=0)) test_set = (test_set - test_set."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "Posts",
          "item": "https://niceasiv.cn/posts/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "KNN\u0026DBscan\u0026K-means实现",
      "item": "https://niceasiv.cn/posts/cluster/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "KNN\u0026DBscan\u0026K-means实现",
  "name": "KNN\u0026DBscan\u0026K-means实现",
  "description": "Python集成开发环境(IDE)\n(1) Anaconda: https://www.continuum.io/ （推荐）\n(2) IDLE: Python解释器默认工具\n(3) PyCharm: https://www.jetbrains.com/pycharm/\n(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。\n需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证\n鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。\n数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是cm，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：\n属性 最大值 最小值 均值 方差 萼长 7.9 4.3 5.84 0.83 萼宽 4.4 2.0 3.05 0.43 瓣长 6.9 1.0 3.76 1.76 瓣宽 2.5 0.1 1.20 0.76 KNN算法 算法原理 存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。\n基本概念 计算距离 常用到的距离计算公式如下：\n欧几里得距离（欧氏距离）：\n$d=\\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$​\n曼哈顿距离\n闵可夫斯基距离\n切比雪夫距离\n马氏距离\n余弦相似度\n皮尔逊相关系数\n汉明距离\n寻找最近邻数据\n将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。 关于K值得选择问题：\nK 值的选择会对算法的结果产生重大影响。 K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。 如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。 在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差） 三种交叉验证方法：\nHold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 决策分类\n明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：\n多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。 说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。\n​\t1）计算待分类点与已知类别的点之间的距离\n​\t2）按照距离递增次序排序\n​\t3）选取与待分类点距离最小的K个点\n​\t4）确定前K个点所在类别的出现次数\n​\t5）返回前K个点出现次数最高的类别作为待分类点的预测分类\n代码实现 初始化数据集 初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。\niris = load_iris() data = iris.data label = iris.target # 划分训练集和测试集 train_set, test_set, train_label, test_label = train_test_split(data, label, test_size=0.2) 数据处理 归一化处理\ntrain_set = (train_set - train_set.min(*axis*=0)) / (train_set.max(*axis*=0) - train_set.min(*axis*=0)) test_set = (test_set - test_set.",
  "keywords": [
    "Machine Learning", "KNN", "DBscan", "K-means"
  ],
  "articleBody": " Python集成开发环境(IDE)\n(1) Anaconda: https://www.continuum.io/ （推荐）\n(2) IDLE: Python解释器默认工具\n(3) PyCharm: https://www.jetbrains.com/pycharm/\n(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。\n需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证\n鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。\n数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是cm，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：\n属性 最大值 最小值 均值 方差 萼长 7.9 4.3 5.84 0.83 萼宽 4.4 2.0 3.05 0.43 瓣长 6.9 1.0 3.76 1.76 瓣宽 2.5 0.1 1.20 0.76 KNN算法 算法原理 存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。\n基本概念 计算距离 常用到的距离计算公式如下：\n欧几里得距离（欧氏距离）：\n$d=\\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$​\n曼哈顿距离\n闵可夫斯基距离\n切比雪夫距离\n马氏距离\n余弦相似度\n皮尔逊相关系数\n汉明距离\n寻找最近邻数据\n将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。 关于K值得选择问题：\nK 值的选择会对算法的结果产生重大影响。 K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。 如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。 在实际应用中，K 值一般选择一个较小的数值，通常采用交叉验证的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差） 三种交叉验证方法：\nHold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。 Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 决策分类\n明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。 最常用的两种决策规则：\n多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。 加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。 说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。\n​\t1）计算待分类点与已知类别的点之间的距离\n​\t2）按照距离递增次序排序\n​\t3）选取与待分类点距离最小的K个点\n​\t4）确定前K个点所在类别的出现次数\n​\t5）返回前K个点出现次数最高的类别作为待分类点的预测分类\n代码实现 初始化数据集 初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。\niris = load_iris() data = iris.data label = iris.target # 划分训练集和测试集 train_set, test_set, train_label, test_label = train_test_split(data, label, test_size=0.2) 数据处理 归一化处理\ntrain_set = (train_set - train_set.min(*axis*=0)) / (train_set.max(*axis*=0) - train_set.min(*axis*=0)) test_set = (test_set - test_set.min(*axis*=0)) / (test_set.max(*axis*=0) - test_set.min(*axis*=0)) 定义距离 def distance(v1, v2): return np.linalg.norm(v1 - v2)#计算两个向量的距离 欧式距离 欧几里得距离$d=\\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$\n创建模型和预测 首先，创建一个Knn实例。然后，在验证集上进行k-fold交叉验证。选择不同的k值，根据验证结果，选择最佳的k值。\ndef predict(train_set, train_label, test_set, k): # 计算测试集中每个样本与训练集中所有样本的距离 distances = [] for i in range(len(train_set)): distances.append(distance(test_set, train_set[i])) # 对距离进行排序 distances = np.array(distances) sort_index = distances.argsort() # 统计前k个样本的标签 class_count = {} for i in range(k): label = train_label[sort_index[i]] class_count[label] = class_count.get(label, 0) + 1 # 返回前k个样本中出现次数最多的标签 max_count = 0 for key, value in class_count.items(): if value \u003e max_count: max_count = value max_index = key return max_index def knn(train_set, train_label, test_set, test_label, k): right_count = 0 for i in range(len(test_set)): predict_label = predict(train_set, train_label, test_set[i], k) if predict_label == test_label[i]: right_count += 1 return right_count / len(test_set) 寻找最佳的邻居数 def find_best_k(train_set, train_label, test_set, test_label): train_accuary = [] test_accuary = [] best_k = 0 #k取值从1到10 for k in range(1, 11): train_accuary.append(knn(train_set, train_label, train_set, train_label, k)) test_accuary.append(knn(train_set, train_label, test_set, test_label, k)) #绘制k值与准确率的关系 plt.plot(range(1, 11), train_accuary, color='red', label='train') plt.plot(range(1, 11), test_accuary, color='blue', label='test') plt.xlabel('k') plt.ylabel('accuary') plt.legend() plt.show() #找到最佳k值 best_k = test_accuary.index(max(test_accuary)) + 1 return best_k 通过可视化分析得知，在n_neighbors取到：4,8，效果还可以，但是推荐使用5，因为综合训练集和测试集，还是不错的\n图形化 def show(train_set, train_label, test_set, test_label, k): # 绘制训练集 plt.title('train set') plt.scatter(train_set[:, 0], train_set[:, 1], c=train_label) plt.show() # 绘制测试集 plt.title('test set') plt.scatter(test_set[:, 0], test_set[:, 1], c=test_label) plt.show() # 绘制预测结果 plt.title('predict result') predict_label = [] for i in range(len(test_set)): predict_label.append(predict(train_set, train_label, test_set[i], k)) plt.scatter(test_set[:, 0], test_set[:, 1], c=predict_label) plt.show() 评估KNN accuracy = knn(train_set, train_label, test_set, test_label, best_k) # 计算准确率 print(accuracy) from sklearn import datasets from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import accuracy_score # 载入数据 iris = datasets.load_iris() # 已经内置了鸢尾花数据集 x = iris.data # 输入4个特征 y = iris.target # 输出类别 # 随机划分数据集，默认25%测试集75%训练集 x_train, x_test, y_train, y_test = train_test_split(x, y) # 创建一个KNN分类器对象，并设置K=5， clf = KNeighborsClassifier(n_neighbors=5) # clf意为Classifier # 训练 clf.fit(x_train, y_train) # 用训练数据拟合分类器模型 # 测试 pre_test = clf.predict(x_test) # 得到测试集的预测结果 # 计算正确率 print('正确率：%.3f' % accuracy_score(y_test, pre_test)) # 由于数据集是随机划分，每次得到正确率自然不同，可以设置random_state让随机一致 #画出预测结果 import matplotlib.pyplot as plt plt.scatter(x_test[:,0],x_test[:,1],c=pre_test) plt.show() 算法评价 优点：\n1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。\n2）可用于非线性分类。\n3）没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练时间复杂度为0。\n4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合。\n5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。\n缺点：\n1）需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。\n2）需要大量的内存，空间复杂度高。\n3）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。\n4）是lazy learning方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。\nDBSCAN算法 算法原理 DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。\n这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。\n基本概念 若给定数据集$D={x_1,x_2,…,x_i}$\n$\\epsilon$-邻域: 对 $\\boldsymbol{x}_j \\in D$, 其 $\\epsilon$-邻域包含样本集 $D$ 中与 $\\boldsymbol{x}j$ 的距离不大于 $\\epsilon$ 的样本,即 $N\\epsilon\\left(\\boldsymbol{x}_j\\right)=\\left{\\boldsymbol{x}_i \\in D \\mid \\operatorname{dist}\\left(\\boldsymbol{x}_i, \\boldsymbol{x}_j\\right) \\leqslant \\epsilon\\right}$;\n1、核心对象：若$x_j$的$\\epsilon$邻域内有超过MinPts个样本，$x_j$是一个核心对象；\n2、密度直达：$x_i$是核心对象，$x_j$位于$x_i$的$\\epsilon$邻域内，则称$x_j$由$x_i$密度直达；\n3、密度可达：对 $\\boldsymbol{x}_i$ 与 $\\boldsymbol{x}_j$, 若存在样本序列 $\\boldsymbol{p}_1, \\boldsymbol{p}_2, \\ldots, \\boldsymbol{p}_n$, 其中 $\\boldsymbol{p}_1=\\boldsymbol{x}_i, \\boldsymbol{p}_n \\doteq \\boldsymbol{x}j$ 且 $\\boldsymbol{p}{i+1}$ 由 $\\boldsymbol{p}_i$ 密度直达, 则称 $\\boldsymbol{x}_j$ 由 $\\boldsymbol{x}_i$ 密度可达;\n4、密度相连：对 $\\boldsymbol{x}_i$ 与 $\\boldsymbol{x}_j$, 若存在 $\\boldsymbol{x}_k$ 使得 $\\boldsymbol{x}_i$ 与 $\\boldsymbol{x}_j$ 均由 $\\boldsymbol{x}_k$ 密度可达, 则称 $\\boldsymbol{x}_i$ 与 $\\boldsymbol{x}_j$ 密度相连.\n核心点 (Core point)。若样本 $x_i$ 的 $\\varepsilon$ 邻域内至少包含了MinPts个样本, 即 $N_{\\varepsilon}\\left(X_i\\right) \\geq$ MinPts，则称样本点 $x_i$ 为核心点。 边界点 (Border point)。若样本 $x_i$ 的 $\\varepsilon$ 邻域内包含的样本数目小于MinPts，但是它在其他核心点的邻域内，则称样本点 $x_i$ 为边界点。 噪音点 (Noise)。既不是核心点也不是边界点的点 簇的定义\nDBSCAN 算法对簇的定义很简单，由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个簇。\n算法流程 输入: 样本集 $\\mathrm{D}=\\left(x_1, x_2, \\ldots, x_m\\right)$ ，邻域参数 $(\\epsilon$, MinPts $)$ ，样本距离度量方式 输出: 簇划分C.\n初始化核心对象集合 $\\Omega=\\emptyset ，$ 初始化聚类簇数 $\\mathrm{k}=0$ ，初始化末访问样本集合 $\\Gamma=\\mathrm{D}$ ，簇划分 $\\mathrm{C}=\\emptyset$ 对于 $j=1,2, \\ldots m$ ，按下面的步㝡找出所有的核心对象: a) 通过距离庹量方式，找到样本 $x_j$ 的 $\\epsilon$-邻域子样本集 $N_\\epsilon\\left(x_j\\right)$ b) 如果子样本集样本个数满足 $\\left|N_\\epsilon\\left(x_j\\right)\\right| \\geq \\operatorname{MinPts}$ ， 将样本 $x_j$ 加入核心对象样本集合: $\\Omega=\\Omega \\cup\\left{x_j\\right}$ 如果核心对象集合 $\\Omega=\\emptyset$ ，则算法结束，否则转入步骤4. 在核心对象集合 $\\Omega$ 中，随机选择一个核心对象 $O$ ，初始化当前笶核心对象队列 $\\Omega_{c u r}={o}$ ，初始化类别序号 $\\mathrm{k}=\\mathrm{k}+1$ ，初始化当前簇样本集合 $C_k={o}$ ，更新末访问样本集合 $\\Gamma=\\Gamma-{o}$ 如果当前笶核心对象队列 $\\Omega_{c u r}=\\emptyset$, 则当前聚类箷 $C_k$ 生成完毕，更新簇划分 $\\mathrm{C}=\\left{C_1, C_2, \\ldots, C_k\\right}$ ，更新核心对象集合 $\\Omega=\\Omega-C_k$ ，转入步 骤3。否则更新核心对象集合 $\\Omega=\\Omega-C_k$ 。 在当前簇核心对象队列 $\\Omega_{c u r}$ 中取出一个核心对象 $o^{\\prime}$, 通过邻域距离阈值 $\\epsilon$ 找出所有的 $\\epsilon$-邻域子样本集 $N_\\epsilon\\left(o^{\\prime}\\right)$ ，令 $\\Delta=N_\\epsilon\\left(o^{\\prime}\\right) \\cap \\Gamma$ ，更新当前簇样本 集合 $C_k=C_k \\cup \\Delta$ ，更新末访问样本集合 $\\Gamma=\\Gamma-\\Delta$ ，更新 $\\Omega_{c u r}=\\Omega_{c u r} \\cup(\\Delta \\cap \\Omega)-o^{\\prime}$ ，转入步噔5. 输出结果为: 簇划分 $\\mathrm{C}=\\left{C_1, C_2, \\ldots, C_k\\right}$ 代码实现 加载数据 iris = load_iris() data = iris.data label = iris.target plt.title('Iris Dataset') plt.scatter(data[:, 0], data[:, 1], c=label) plt.show() 计算距离 这里的距离度量是用的二范数或者欧几里得范数\nThe $L_2$-norm (or 2-norm, or Euclidean norm) $$ |\\boldsymbol{x}|2=\\sqrt{\\sum{i=1}^n x_i^2} $$ 然后下方这个函数就主要负责距离的查询\n选择每次的首个核心点，并传入参数epsilon和min_points进行首次迭代\ndef region_query(data, point_id, eps): # Find all points within eps distance of point neighbors = [] for i in range(0, len(data)): if np.linalg.norm(data[i] - data[point_id]) \u003c eps: neighbors.append(i) return neighbors 寻找近邻点 首个核心点迭代完成后，对它进行移动，直到出现不满足阈值条件的样本点为止\ndef expandCluster(data,label,point_id,cluster_id,eps,min_points): # 返回所有近邻点 neighbors = region_query(data,point_id, eps) # print(neighbors) # 如果点的密度小于min_points，则为噪声点 if len(neighbors) \u003c min_points: label[point_id] = -1 return False # 如果点的密度大于min_points，则为核心点 else: # 为核心点赋新簇标签 label[point_id] = cluster_id for neighbor in neighbors: label[neighbor] = cluster_id#为eps半径内的点赋新簇标签 # 遍历所有近邻点 while len(neighbors) \u003e 0:#对近邻点进行扩展 current_point = neighbors[0]#取出第一个点 query_results = region_query(data,current_point, eps)#找出该点的所有近邻点 if len(query_results) \u003e= min_points:#如果该点的密度大于min_points，则为核心点 for i in range(0, len(query_results)):#遍历所有近邻点 result_point = query_results[i] if label[result_point] == -1:#如果该点为噪声点，则赋新簇标签 label[result_point] = cluster_id elif label[result_point] == 0:#如果该点未访问过，则赋新簇标签，并加入neighbors label[result_point] = cluster_id neighbors.append(result_point) neighbors.remove(current_point)#删除当前点 return True dbscan 初始化一个空的分类列表，对其中每个未分类点进行调用上述函数\ndef dbscan(data,eps,min_points): cluster_id = 1 label = [0] * len(data)#初始化一个空的分类列表 for point_id in range(0, len(data)): if label[point_id] == 0:#如果该点未访问过 if expandCluster(data,label,point_id,cluster_id,eps,min_points): #可变类型：类似 c++ 的引用传递，如 列表，字典，类等 cluster_id = cluster_id + 1 print(cluster_id) return label 可视化模型结果 按照前两个参数进行画图\nplt.title('DBSCAN') plt.scatter(data[:, 0], data[:, 1], c=label_pred) plt.xlabel('sepal length') plt.ylabel('sepal width') #图例 plt.show() 评估K-means模型 print(\"轮廓系数为:\",metrics.silhouette_score(data, label_pred, metric='euclidean'))\n与sklearn的对比\nfrom sklearn.cluster import DBSCAN from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import numpy as np import matplotlib.pyplot as plt from sklearn import metrics iris=load_iris() data=iris.data label=iris.target db = DBSCAN(eps=0.46, min_samples=11).fit(data) skl_labels = db.labels_ plt.title('dbscan sklearn') plt.scatter(data[:, 0], data[:, 1], c=skl_labels) plt.show() 完整代码 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split import numpy as np import matplotlib.pyplot as plt def region_query(data, point_id, eps): # Find all points within eps distance of point neighbors = [] for i in range(0, len(data)): if np.linalg.norm(data[i] - data[point_id]) \u003c eps: neighbors.append(i) return neighbors def expandCluster(data,label,point_id,cluster_id,eps,min_points): # 返回所有近邻点 neighbors = region_query(data,point_id, eps) # print(neighbors) # 如果点的密度小于min_points，则为噪声点 if len(neighbors) \u003c min_points: label[point_id] = -1 return False # 如果点的密度大于min_points，则为核心点 else: # 为核心点赋新簇标签 label[point_id] = cluster_id for neighbor in neighbors: label[neighbor] = cluster_id#为eps半径内的点赋新簇标签 # 遍历所有近邻点 while len(neighbors) \u003e 0:#对近邻点进行扩展 current_point = neighbors[0]#取出第一个点 query_results = region_query(data,current_point, eps)#找出该点的所有近邻点 if len(query_results) \u003e= min_points:#如果该点的密度大于min_points，则为核心点 for i in range(0, len(query_results)):#遍历所有近邻点 result_point = query_results[i] if label[result_point] == -1:#如果该点为噪声点，则赋新簇标签 label[result_point] = cluster_id elif label[result_point] == 0:#如果该点未访问过，则赋新簇标签，并加入neighbors label[result_point] = cluster_id neighbors.append(result_point) neighbors.remove(current_point)#删除当前点 return True #初始化一个空的分类列表，对其中每个未分类点进行调用上述函数 def dbscan(data,eps,min_points): cluster_id = 1 label = [0] * len(data)#初始化一个空的分类列表 for point_id in range(0, len(data)): if label[point_id] == 0:#如果该点未访问过 if expandCluster(data,label,point_id,cluster_id,eps,min_points): #可变类型：类似 c++ 的引用传递，如 列表，字典，类等 cluster_id = cluster_id + 1 print(cluster_id) return label def showCluster(data, label): n_clusters_ = len(set(label)) - (1 if -1 in label else 0) print(n_clusters_) # Black removed and is used for noise instead. unique_labels = set(label) colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))] for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = [0, 0, 0, 1] class_member_mask = (label == k) xy = data[class_member_mask] plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14) plt.title('Estimated number of clusters: %d' % n_clusters_) plt.show() if __name__ == '__main__': iris = load_iris() data = iris.data label = iris.target plt.title('Iris Dataset') plt.scatter(data[:, 0], data[:, 1], c=label) plt.show() label_pred = dbscan(data,0.46,11) print(label_pred) # print(label_pred) plt.title('DBSCAN') plt.scatter(data[:, 0], data[:, 1], c=label_pred) #图例 plt.show() 算法评价 优点：\n不需要设置k值 可以发现任意形状的蔟 可以聚类的同时发现噪音点，即对噪音不敏感 对样本输入顺序不敢兴趣 缺点：\n高维数据效果不理想 调参复杂，eps和Minpiont参数不好设置，无法预估。 K-means算法 算法原理 K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。\nk-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。\n算法流程 算法接受一个未标记的数据集，然后将数据聚类成不同的组。假设将数据分成k个组，方法为：\n选择初始化的 $\\mathrm{k}$ 个样本作为初始聚类中心 $a=a_1, a_2, \\ldots a_k$ ； 针对数据集中每个样本 $x_i$ 计算它到 $\\mathrm{k}$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应 的类中； 针对每个类别 $a_j$ ，重新计算它的聚类中心 $a_j=\\frac{1}{\\left|c_i\\right|} \\sum_{x \\in c_i} x$ (即属于该类的所有样本的质 心）； 重复上面 23 两步操作，直到达到某个中止条件 (迭代次数、最小误差变化等)。 吴恩达视频的中的伪代码为\nrepeat { for i= to m # 计算每个样例属于的类 c(i) := index (from 1 to K) of cluster centroid closest to x(i) for k = 1 to K # 聚类中心的移动，重新计算该类的质心 u(k) := average (mean) of points assigned to cluster K } 优化目标 这里的终止条件，我们用K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此K-均值的代价函数（畸变函数Distortion function） ： $$ \\begin{equation} J\\left(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots, \\mu_K\\right)=\\frac{1}{m} \\sum_{i=1}^m\\left|X^{(i)}-\\mu_{c^{(i)}}\\right|^2 \\end{equation} $$ 其中$\\mu$代表与$x_i$最近的聚类中心点,$c^i$代表族类\n优化目标就是找出使得代价函数最小的c和μ，即 $$ \\begin{aligned} J\\left(\\underline{c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots, \\mu_K}\\right)=\\frac{1}{m} \\sum_{i=1}^m\\left|x^{(i)}-\\mu_{c^{(i)}}\\right|^2\\ \\min _{\\substack{c^{(1)}, \\ldots, c^{(m)}\\ \\mu_1, \\ldots, \\mu_K}} J\\left(c^{(1)}, \\ldots, c^{(m)}, \\mu_1, \\ldots, \\mu_K\\right) \\ \u0026 \\end{aligned} $$\n代码实现 加载数据 我们用Python的scikit-learn库中自带的鸢尾花数据集，并使用使用datasets.load_iris()载入，并用开头的两个维度进行画图\n#加载数据 iris = load_iris() #划分训练集和测试集 X_data, lable_data = iris.data, iris.target def showData(X_data, lable_data): #显示数据 plt.scatter(X_data[:, 0], X_data[:, 1], c=lable_data) plt.xlabel('sepal length') plt.ylabel('sepal width') plt.show() 数据处理 不同特征之间往往具有不同的量纲，由此所造成的数值间的差异可能很大，在涉及空间距离计算或梯度下降法等情况的时候不对其进行处理会影响到数据分析结果的准确性。为了消除特征之间的量纲和取值范围差异可能会造成的影响，需对数据进行标准化处理，也可以称为规范化处理。 在这里我们对数据集进行标准差标准化处理\nMMS = MinMaxScaler().fit(X_data) X_data = MMS.transform(X_data) 计算距离 初始化质心，我们选取k个样本作为初始的类的中心\ndef initCentroids(X_train, k): #初始化质心 #随机选取k个样本作为初始质心 numSamples, dim = X_train.shape centroids = np.zeros((k, dim)) for i in range(k): index = int(np.random.uniform(0, numSamples)) centroids[i, :] = X_train[index, :] return centroids 计算每个样本到类中心的距离，并选取其中欧拉距离最小的，并打上属于某个类的标记\ndef OulerDistance(vec1, vec2): #欧拉距离 #计算两个向量的欧拉距离 :math:`d = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}` #vec1-vec2表示两个向量的对应元素的差 return np.sqrt(np.sum(np.square(vec1 - vec2))) def minDistance(X_train, centroids): #计算每个样本到质心的距离 #返回每个样本所属的簇 numSamples = X_train.shape[0] clusterDict = dict() k=centroids.shape[0] for flower in X_train: vec1=flower flag = -1 minDist = float('inf')#无穷大 for i in range(k): vec2=centroids[i] distance = OulerDistance(vec1, vec2) if distance \u003c minDist: minDist = distance flag = i#标记为第i个簇 if flag not in clusterDict.keys(): clusterDict[flag] = [] clusterDict[flag].append(flower)#将该样本加入到第i个簇中 return clusterDict#返回每个样本所属的簇 求两个向量的欧拉距离: $$ d = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2} $$ 获取更新新的聚类中心\ndef getCentroids(clusterDict): #计算新的质心 #返回新的质心 centroids = np.zeros((len(clusterDict),len(clusterDict[0][0]))) for i, cluster in clusterDict.items(): cluster = np.array(cluster) centroids[i, :] = np.mean(cluster, axis=0) return centroids def getVaration(clusterDict, centroids): #计算簇内误差平方和 #返回簇内误差平方和 variation = 0.0 for i, cluster in clusterDict.items(): variation += np.sum(np.square(cluster - centroids[i, :])) return variation 针对每个类别 $a_j$,重新计算它的聚类中心 $a_j=\\frac{1}{\\left|c_i\\right|} \\sum_{x \\in c_i} x$ (即属于该类的所有样本的质心）；\n寻找最小mean new_variation = getVaration(clusterDict, centroids) #显示图形 old_variation = 1 while abs(new_variation - old_variation) \u003e 0.0001: old_variation = new_variation centroids = getCentroids(clusterDict) clusterDict = minDistance(X_data, centroids) new_variation = getVaration(clusterDict, centroids) 可视化模型 根据前两个维度画出散点图\ndef showCluster(centroids, clusterDict): #获取聚类标签 labels0 = np.array(clusterDict[0]) labels1 = np.array(clusterDict[1]) labels2 = np.array(clusterDict[2]) #绘制样本点 plt.scatter(labels0[:, 0], labels0[:, 1], marker='x', color='r', label='label0') plt.scatter(labels1[:, 0], labels1[:, 1], marker='o', color='g', label='label1') plt.scatter(labels2[:, 0], labels2[:, 1], marker='*', color='b', label='label2') plt.xlabel('sepal length') plt.ylabel('sepal width') #绘制质心 plt.scatter(centroids[:, 0], centroids[:, 1], marker='+', color='black', label='centroids', s=300) plt.legend(loc='upper right') plt.title('the result of K-means') plt.show() 评估K-means模型 print('轮廓系数为：', silhouette_score(X_data, lable_data, metric='euclidean'))\n为了作对比这里直接调用sklearn库进行模拟，方便我们分析，对比\n在这里我们获取轮廓系数score是所有样本的轮廓系数均值，如果要获取每个样本的轮廓系数应当使用silhouette_samples。这里是针对超参数k(n_cluster)，所以采用轮廓系数均值进行评估。\nfrom sklearn.metrics import silhouette_score from sklearn.preprocessing import MinMaxScaler from sklearn.cluster import KMeans from sklearn.datasets import load_iris import matplotlib.pyplot as plt iris_data = load_iris() x = iris_data.data y = iris_data.target MMS = MinMaxScaler().fit(x) data = MMS.transform(x) #构建KMeans模型训练数据 cluster = KMeans(n_clusters=3,random_state=123).fit(data) #获取聚类结果 y_pred = cluster.labels_ #获取聚类中心 centers = cluster.cluster_centers_ #获取聚类的内部误差平方和 inertia = cluster.inertia_ #计算轮廓系数 print('轮廓系数为：', silhouette_score(data, y_pred, metric='euclidean')) #显示聚类结果 plt.scatter(data[:, 0], data[:, 1], c=y_pred) plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='*') plt.show() silhouetteScore = [] for i in range(2,15): # 构建并训练模型 kmeans = KMeans(n_clusters=i,random_state=123).fit(data) score = silhouette_score(data,kmeans.labels_) silhouetteScore.append(score) plt.figure(figsize=(10,6)) plt.plot(range(2,15),silhouetteScore,linewidth=1.5,linestyle='-') plt.xlabel('the number of clusters') plt.ylabel('silhouette coefficient') plt.show() 可以看到聚类数目为2、3和4、5的时候，图形的畸变程度最大。本身数据集就是关于3种鸢尾花的，侧面说明了聚类为3的时候效果较好，且用库的和实现的效果一致\n完整代码 from sklearn.datasets import load_iris from sklearn.preprocessing import MinMaxScaler from sklearn.manifold import TSNE from sklearn.metrics import silhouette_score import numpy as np import matplotlib.pyplot as plt def showData(X_data, lable_data): #显示数据 plt.scatter(X_data[:, 0], X_data[:, 1], c=lable_data) plt.xlabel('sepal length') plt.ylabel('sepal width') plt.title('the original data') plt.show() def showCluster(centroids, clusterDict): #获取聚类标签 labels0 = np.array(clusterDict[0]) labels1 = np.array(clusterDict[1]) labels2 = np.array(clusterDict[2]) #绘制样本点 plt.scatter(labels0[:, 0], labels0[:, 1], marker='x', color='r', label='label0') plt.scatter(labels1[:, 0], labels1[:, 1], marker='o', color='g', label='label1') plt.scatter(labels2[:, 0], labels2[:, 1], marker='*', color='b', label='label2') plt.xlabel('sepal length') plt.ylabel('sepal width') #绘制质心 plt.scatter(centroids[:, 0], centroids[:, 1], marker='+', color='black', label='centroids', s=300) plt.legend(loc='upper right') plt.title('the result of K-means') plt.show() def OulerDistance(vec1, vec2): #欧拉距离 #计算两个向量的欧拉距离 :math:`d = \\sqrt{\\sum_{i=1}^n (x_i - y_i)^2}` #vec1-vec2表示两个向量的对应元素的差 return np.sqrt(np.sum(np.square(vec1 - vec2))) def initCentroids(X_train, k): #初始化质心 #随机选取k个样本作为初始质心 numSamples, dim = X_train.shape centroids = np.zeros((k, dim)) for i in range(k): index = int(np.random.uniform(0, numSamples)) centroids[i, :] = X_train[index, :] return centroids def minDistance(X_train, centroids): #计算每个样本到质心的距离 #返回每个样本所属的簇 numSamples = X_train.shape[0] clusterDict = dict() k=centroids.shape[0] for flower in X_train: vec1=flower flag = -1 minDist = float('inf')#无穷大 for i in range(k): vec2=centroids[i] distance = OulerDistance(vec1, vec2) if distance \u003c minDist: minDist = distance flag = i#标记为第i个簇 if flag not in clusterDict.keys(): clusterDict[flag] = [] clusterDict[flag].append(flower)#将该样本加入到第i个簇中 return clusterDict#返回每个样本所属的簇 def getCentroids(clusterDict): #计算新的质心 #返回新的质心 centroids = np.zeros((len(clusterDict),len(clusterDict[0][0]))) for i, cluster in clusterDict.items(): cluster = np.array(cluster) centroids[i, :] = np.mean(cluster, axis=0) return centroids def getVaration(clusterDict, centroids): #计算簇内误差平方和 #返回簇内误差平方和 variation = 0.0 for i, cluster in clusterDict.items(): variation += np.sum(np.square(cluster - centroids[i, :])) return variation def Kmeans(): #加载数据 iris = load_iris() #划分训练集和测试集 X_data, lable_data = iris.data, iris.target # showData(X_data, lable_data) #数据归一化 MMS = MinMaxScaler().fit(X_data) X_data = MMS.transform(X_data) #初始化质心 k = 3 centroids = initCentroids(X_data, k) #计算每个样本到质心的距离 clusterDict = minDistance(X_data, centroids) #计算新的质心 centroids = getCentroids(clusterDict) #计算簇内误差平方和 new_variation = getVaration(clusterDict, centroids) #显示图形 old_variation = 1 while abs(new_variation - old_variation) \u003e 0.0001: old_variation = new_variation centroids = getCentroids(clusterDict) clusterDict = minDistance(X_data, centroids) new_variation = getVaration(clusterDict, centroids) showCluster(centroids, clusterDict) #计算轮廓系数 print('轮廓系数为：', silhouette_score(X_data, lable_data, metric='euclidean')) if __name__ == '__main__': Kmeans() 算法评价 优点：\n1.是解决聚类问题的一种经典算法，简单、快速\n2.对处理大数据集，该算法保持可伸缩性和高效率\n3.当结果簇是密集的，它的效果较好\n缺点\n1.在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用\n2.必须事先给出k（要生成的簇的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果。\n3.不适合于发现非凸形状的簇或者大小差别很大的簇\n4.对躁声和孤立点数据敏感\n参考文献 Supervised Machine Learning: Regression and Classification\nhttps://blog.51cto.com/u_15749390/5570555\nhttps://zhuanlan.zhihu.com/p/53084915\n",
  "wordCount" : "2042",
  "inLanguage": "zh",
  "datePublished": "2022-11-22T20:51:09Z",
  "dateModified": "2022-11-22T20:51:09Z",
  "author":{
    "@type": "Person",
    "name": "Asiv"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niceasiv.cn/posts/cluster/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Asiv's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niceasiv.cn/img/logo.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niceasiv.cn" accesskey="h" title="Asiv&#39;s Blog (Alt + H)">
            <img src="https://niceasiv.cn/img/logo.jpg" alt="logo" aria-label="logo"
                 height="35">Asiv&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niceasiv.cn/search/" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                <span>🔍 搜索</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/" title="🏠 主页">
                <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/posts" title="📚 文章">
                <span>📚 文章</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/tags" title="🧩 标签">
                <span>🧩 标签</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/archives/" title="⏱️ 时间轴">
                <span>⏱️ 时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/friends/" title="🤝 友链">
                <span>🤝 友链</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://niceasiv.cn">🏠 主页</a>&nbsp;»&nbsp;<a href="https://niceasiv.cn/posts/">Posts</a></div>
            <h1 class="post-title">
                KNN&amp;DBscan&amp;K-means实现
            </h1>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>2022-11-22
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>2042字
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>10分钟
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Asiv
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://niceasiv.cn/tags/machine-learning/" style="color: var(--secondary)!important;">Machine Learning</a>
                &nbsp;<a href="https://niceasiv.cn/tags/knn/" style="color: var(--secondary)!important;">KNN</a>
                &nbsp;<a href="https://niceasiv.cn/tags/dbscan/" style="color: var(--secondary)!important;">DBscan</a>
                &nbsp;<a href="https://niceasiv.cn/tags/k-means/" style="color: var(--secondary)!important;">K-means</a>
            </span>
            &nbsp;&nbsp;
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://niceasiv.cn"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "https://twikoocdn.vercel.app/", 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#knn%e7%ae%97%e6%b3%95" aria-label="KNN算法">KNN算法</a><ul>
                        
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86" aria-label="算法原理">算法原理</a><ul>
                        
                <li>
                    <a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5" aria-label="基本概念">基本概念</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0" aria-label="代码实现">代码实现</a><ul>
                        
                <li>
                    <a href="#%e5%88%9d%e5%a7%8b%e5%8c%96%e6%95%b0%e6%8d%ae%e9%9b%86" aria-label="初始化数据集">初始化数据集</a></li>
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86" aria-label="数据处理">数据处理</a></li>
                <li>
                    <a href="#%e5%ae%9a%e4%b9%89%e8%b7%9d%e7%a6%bb" aria-label="定义距离">定义距离</a></li>
                <li>
                    <a href="#%e5%88%9b%e5%bb%ba%e6%a8%a1%e5%9e%8b%e5%92%8c%e9%a2%84%e6%b5%8b" aria-label="创建模型和预测">创建模型和预测</a></li>
                <li>
                    <a href="#%e5%af%bb%e6%89%be%e6%9c%80%e4%bd%b3%e7%9a%84%e9%82%bb%e5%b1%85%e6%95%b0" aria-label="寻找最佳的邻居数"><strong>寻找最佳的邻居数</strong></a></li>
                <li>
                    <a href="#%e5%9b%be%e5%bd%a2%e5%8c%96" aria-label="图形化">图形化</a></li>
                <li>
                    <a href="#%e8%af%84%e4%bc%b0knn" aria-label="评估KNN">评估KNN</a></li>
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e8%af%84%e4%bb%b7" aria-label="算法评价">算法评价</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#dbscan%e7%ae%97%e6%b3%95" aria-label="DBSCAN算法">DBSCAN算法</a><ul>
                        
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86-1" aria-label="算法原理">算法原理</a><ul>
                        
                <li>
                    <a href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5-1" aria-label="基本概念">基本概念</a></li>
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b" aria-label="算法流程">算法流程</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0-1" aria-label="代码实现">代码实现</a><ul>
                        
                <li>
                    <a href="#%e5%8a%a0%e8%bd%bd%e6%95%b0%e6%8d%ae" aria-label="加载数据">加载数据</a></li>
                <li>
                    <a href="#%e8%ae%a1%e7%ae%97%e8%b7%9d%e7%a6%bb" aria-label="计算距离">计算距离</a></li>
                <li>
                    <a href="#%e5%af%bb%e6%89%be%e8%bf%91%e9%82%bb%e7%82%b9" aria-label="寻找近邻点">寻找近邻点</a></li>
                <li>
                    <a href="#dbscan" aria-label="dbscan">dbscan</a></li>
                <li>
                    <a href="#%e5%8f%af%e8%a7%86%e5%8c%96%e6%a8%a1%e5%9e%8b%e7%bb%93%e6%9e%9c" aria-label="可视化模型结果">可视化模型结果</a></li>
                <li>
                    <a href="#%e8%af%84%e4%bc%b0k-means%e6%a8%a1%e5%9e%8b" aria-label="评估K-means模型">评估K-means模型</a></li>
                <li>
                    <a href="#%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81" aria-label="完整代码">完整代码</a></li>
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e8%af%84%e4%bb%b7-1" aria-label="算法评价">算法评价</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#k-means%e7%ae%97%e6%b3%95" aria-label="K-means算法">K-means算法</a><ul>
                        
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86-2" aria-label="算法原理">算法原理</a><ul>
                        
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e6%b5%81%e7%a8%8b-1" aria-label="算法流程">算法流程</a></li>
                <li>
                    <a href="#%e4%bc%98%e5%8c%96%e7%9b%ae%e6%a0%87" aria-label="优化目标">优化目标</a></li></ul>
                </li>
                <li>
                    <a href="#%e4%bb%a3%e7%a0%81%e5%ae%9e%e7%8e%b0-2" aria-label="代码实现">代码实现</a><ul>
                        
                <li>
                    <a href="#%e5%8a%a0%e8%bd%bd%e6%95%b0%e6%8d%ae-1" aria-label="加载数据">加载数据</a></li>
                <li>
                    <a href="#%e6%95%b0%e6%8d%ae%e5%a4%84%e7%90%86-1" aria-label="数据处理">数据处理</a></li>
                <li>
                    <a href="#%e8%ae%a1%e7%ae%97%e8%b7%9d%e7%a6%bb-1" aria-label="计算距离">计算距离</a></li>
                <li>
                    <a href="#%e5%af%bb%e6%89%be%e6%9c%80%e5%b0%8fmean" aria-label="寻找最小mean">寻找最小mean</a></li>
                <li>
                    <a href="#%e5%8f%af%e8%a7%86%e5%8c%96%e6%a8%a1%e5%9e%8b" aria-label="可视化模型">可视化模型</a></li>
                <li>
                    <a href="#%e8%af%84%e4%bc%b0k-means%e6%a8%a1%e5%9e%8b-1" aria-label="评估K-means模型">评估K-means模型</a></li>
                <li>
                    <a href="#%e5%ae%8c%e6%95%b4%e4%bb%a3%e7%a0%81-1" aria-label="完整代码">完整代码</a></li>
                <li>
                    <a href="#%e7%ae%97%e6%b3%95%e8%af%84%e4%bb%b7-2" aria-label="算法评价">算法评价</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" aria-label="参考文献">参考文献</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><blockquote>
<p>Python集成开发环境(IDE)</p>
<p>(1) Anaconda: <a href="https://www.continuum.io/">https://www.continuum.io/</a> （推荐）</p>
<p>(2) IDLE: Python解释器默认工具</p>
<p>(3) PyCharm: <a href="https://www.jetbrains.com/pycharm/">https://www.jetbrains.com/pycharm/</a></p>
<p>(4) 实验数据集：Python的scikit-learn库中自带的鸢尾花数据集，可使用datasets.load_iris()载入。</p>
</blockquote>
<p><code>需要描述清楚算法流程，包括加载数据、数据处理、创建模型，训练，预测，评估模型等。如果必须给出实现代码才能更好地说明问题时，也必须先有相关的文字叙述，然后才是代码，代码只是作为例证</code></p>
<p>鸢尾花数据集共收集了三类鸢尾花，即Setosa鸢尾花、Versicolour鸢尾花和Virginica鸢尾花，每一类鸢尾花收集了50条样本记录，共计150条。</p>
<p>数据集包括4个属性，分别为花萼的长、花萼的宽、花瓣的长和花瓣的宽。对花瓣我们可能比较熟悉，花萼是什么呢？花萼是花冠外面的绿色被叶，在花尚未开放时，保护着花蕾。四个属性的单位都是<code>cm</code>，属于数值变量，四个属性均不存在缺失值的情况，以下是各属性的一些统计值如下：</p>
<table>
<thead>
<tr>
<th style="text-align:center">属性</th>
<th style="text-align:center">最大值</th>
<th style="text-align:center">最小值</th>
<th style="text-align:center">均值</th>
<th style="text-align:center">方差</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">萼长</td>
<td style="text-align:center">7.9</td>
<td style="text-align:center">4.3</td>
<td style="text-align:center">5.84</td>
<td style="text-align:center">0.83</td>
</tr>
<tr>
<td style="text-align:center">萼宽</td>
<td style="text-align:center">4.4</td>
<td style="text-align:center">2.0</td>
<td style="text-align:center">3.05</td>
<td style="text-align:center">0.43</td>
</tr>
<tr>
<td style="text-align:center">瓣长</td>
<td style="text-align:center">6.9</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">3.76</td>
<td style="text-align:center">1.76</td>
</tr>
<tr>
<td style="text-align:center">瓣宽</td>
<td style="text-align:center">2.5</td>
<td style="text-align:center">0.1</td>
<td style="text-align:center">1.20</td>
<td style="text-align:center">0.76</td>
</tr>
</tbody>
</table>
<h2 id="knn算法">KNN算法<a hidden class="anchor" aria-hidden="true" href="#knn算法">#</a></h2>
<h3 id="算法原理">算法原理<a hidden class="anchor" aria-hidden="true" href="#算法原理">#</a></h3>
<p>存在一个样本数据集合，也称为训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类对应的关系。输入没有标签的数据后，将新数据中的每个特征与样本集中数据对应的特征进行比较，提取出样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k近邻算法中k的出处，通常k是不大于20的整数。最后选择k个最相似数据中出现次数最多的分类作为新数据的分类。</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/20210526131701159.png" alt="by demo"  />
</p>
<h4 id="基本概念">基本概念<a hidden class="anchor" aria-hidden="true" href="#基本概念">#</a></h4>
<p><strong>计算距离</strong>
常用到的距离计算公式如下：</p>
<ol>
<li>
<p>欧几里得距离（欧氏距离）：</p>
<p>$d=\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$​</p>
</li>
<li>
<p>曼哈顿距离</p>
</li>
<li>
<p>闵可夫斯基距离</p>
</li>
<li>
<p>切比雪夫距离</p>
</li>
<li>
<p>马氏距离</p>
</li>
<li>
<p>余弦相似度</p>
</li>
<li>
<p>皮尔逊相关系数</p>
</li>
<li>
<p>汉明距离</p>
</li>
</ol>
<p><strong>寻找最近邻数据</strong></p>
<p>将所有距离进行升序排序，确定K值，最近的K个邻居即距离最短的K个数据。
<strong>关于K值得选择问题：</strong></p>
<ul>
<li>K 值的选择会对算法的结果产生重大影响。</li>
<li>K值较小意味着只有与测试数据较近的训练实例才会对预测结果起作用，容易发生过拟合。</li>
<li>如果 K 值较大，优点是可以减少学习的估计误差，但缺点是学习的近似误差增大，这时与测试数据较远的训练实例也会对预测起作用，使预测发生错误。</li>
<li>在实际应用中，K 值一般选择一个较小的数值，通常采用<strong>交叉验证</strong>的方法来选择最优的 K 值。随着训练实例数目趋向于无穷和 K=1 时，误差率不会超过贝叶斯误差率的2倍，如果K也趋向于无穷，则误差率趋向于贝叶斯误差率。（贝叶斯误差可以理解为最小误差）</li>
</ul>
<p><strong>三种交叉验证方法</strong>：</p>
<ul>
<li>Hold-Out： 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。常识来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。</li>
<li>K-foldcross-validation：K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</li>
<li>Leave-One-Out Cross Validation：正如名称所建议， 留一验证(LOOCV)意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。</li>
</ul>
<p><strong>决策分类</strong></p>
<p>明确K个邻居中所有数据类别的个数，将测试数据划分给个数最多的那一类。即由输入实例的 K 个最临近的训练实例中的多数类决定输入实例的类别。
最常用的两种决策规则：</p>
<ul>
<li>多数表决法：多数表决法和我们日常生活中的投票表决是一样的，少数服从多数，是最常用的一种方法。</li>
<li>加权表决法：有些情况下会使用到加权表决法，比如投票的时候裁判投票的权重更大，而一般人的权重较小。所以在数据之间有权重的情况下，一般采用加权表决法。</li>
</ul>
<p>说明：KNN没有显示的训练过程，它是“懒惰学习”的代表，它在训练阶段只是把数据保存下来，训练时间开销为0，等收到测试样本后进行处理。</p>
<p>​	1）计算待分类点与已知类别的点之间的距离</p>
<p>​	2）按照距离递增次序排序</p>
<p>​	3）选取与待分类点距离最小的K个点</p>
<p>​	4）确定前K个点所在类别的出现次数</p>
<p>​	5）返回前K个点出现次数最高的类别作为待分类点的预测分类</p>
<h3 id="代码实现">代码实现<a hidden class="anchor" aria-hidden="true" href="#代码实现">#</a></h3>
<h4 id="初始化数据集">初始化数据集<a hidden class="anchor" aria-hidden="true" href="#初始化数据集">#</a></h4>
<p>初始化训练集和测试集。训练集一般为两类或者多种类别的数据；测试集一般为一个数据。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_label</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="数据处理">数据处理<a hidden class="anchor" aria-hidden="true" href="#数据处理">#</a></h4>
<p>归一化处理</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">train_set</span> <span class="o">=</span> <span class="p">(</span><span class="n">train_set</span> <span class="o">-</span> <span class="n">train_set</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">train_set</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">train_set</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">  <span class="n">test_set</span> <span class="o">=</span> <span class="p">(</span><span class="n">test_set</span> <span class="o">-</span> <span class="n">test_set</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">test_set</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_set</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="o">*</span><span class="n">axis</span><span class="o">*=</span><span class="mi">0</span><span class="p">))</span>
</span></span></code></pre></div><h4 id="定义距离">定义距离<a hidden class="anchor" aria-hidden="true" href="#定义距离">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">v1</span> <span class="o">-</span> <span class="n">v2</span><span class="p">)</span><span class="c1">#计算两个向量的距离 欧式距离</span>
</span></span></code></pre></div><p>欧几里得距离$d=\sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}$</p>
<h4 id="创建模型和预测">创建模型和预测<a hidden class="anchor" aria-hidden="true" href="#创建模型和预测">#</a></h4>
<p>首先，创建一个Knn实例。然后，在验证集上进行k-fold交叉验证。选择不同的k值，根据验证结果，选择最佳的k值。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 计算测试集中每个样本与训练集中所有样本的距离</span>
</span></span><span class="line"><span class="cl">    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_set</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">distance</span><span class="p">(</span><span class="n">test_set</span><span class="p">,</span> <span class="n">train_set</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 对距离进行排序</span>
</span></span><span class="line"><span class="cl">    <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">sort_index</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">argsort</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 统计前k个样本的标签</span>
</span></span><span class="line"><span class="cl">    <span class="n">class_count</span> <span class="o">=</span> <span class="p">{}</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span> <span class="o">=</span> <span class="n">train_label</span><span class="p">[</span><span class="n">sort_index</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl">        <span class="n">class_count</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="n">class_count</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 返回前k个样本中出现次数最多的标签</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_count</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">class_count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">max_count</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_count</span> <span class="o">=</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">            <span class="n">max_index</span> <span class="o">=</span> <span class="n">key</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">max_index</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">test_label</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">right_count</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">predict_label</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">predict_label</span> <span class="o">==</span> <span class="n">test_label</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
</span></span><span class="line"><span class="cl">            <span class="n">right_count</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">right_count</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="寻找最佳的邻居数"><strong>寻找最佳的邻居数</strong><a hidden class="anchor" aria-hidden="true" href="#寻找最佳的邻居数">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">find_best_k</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">test_label</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">train_accuary</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">test_accuary</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_k</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#k取值从1到10</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">train_accuary</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">test_accuary</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knn</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">test_label</span><span class="p">,</span> <span class="n">k</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#绘制k值与准确率的关系</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">train_accuary</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">),</span> <span class="n">test_accuary</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;accuary&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#找到最佳k值</span>
</span></span><span class="line"><span class="cl">    <span class="n">best_k</span> <span class="o">=</span> <span class="n">test_accuary</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">test_accuary</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">best_k</span>
</span></span></code></pre></div><p>通过可视化分析得知，在n_neighbors取到：4,8，效果还可以，但是推荐使用5，因为综合训练集和测试集，还是不错的</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.1.png" alt="Figure_3.1"  />
</p>
<h4 id="图形化">图形化<a hidden class="anchor" aria-hidden="true" href="#图形化">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">show</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">test_label</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 绘制训练集</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;train set&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">train_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">train_label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 绘制测试集</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;test set&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">test_label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 绘制预测结果</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;predict result&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">predict_label</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_set</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">predict_label</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">k</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_set</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">test_set</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">predict_label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.2.png" alt="Figure_3.2"  />
</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.3.png" alt="Figure_3.3"  />
</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.4.png" alt="Figure_3.4"  />
</p>
<h4 id="评估knn">评估KNN<a hidden class="anchor" aria-hidden="true" href="#评估knn">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="n">accuracy</span> <span class="o">=</span> <span class="n">knn</span><span class="p">(</span><span class="n">train_set</span><span class="p">,</span> <span class="n">train_label</span><span class="p">,</span> <span class="n">test_set</span><span class="p">,</span> <span class="n">test_label</span><span class="p">,</span> <span class="n">best_k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"> <span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"> <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221123235522305.png" alt="image-20221123235522305"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 载入数据</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>  <span class="c1"># 已经内置了鸢尾花数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># 输入4个特征</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>  <span class="c1"># 输出类别</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 随机划分数据集，默认25%测试集75%训练集</span>
</span></span><span class="line"><span class="cl"><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建一个KNN分类器对象，并设置K=5，</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  <span class="c1"># clf意为Classifier</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 训练</span>
</span></span><span class="line"><span class="cl"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>  <span class="c1"># 用训练数据拟合分类器模型</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 测试</span>
</span></span><span class="line"><span class="cl"><span class="n">pre_test</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>  <span class="c1"># 得到测试集的预测结果</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算正确率</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;正确率：</span><span class="si">%.3f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pre_test</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 由于数据集是随机划分，每次得到正确率自然不同，可以设置random_state让随机一致</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#画出预测结果</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">x_test</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">c</span><span class="o">=</span><span class="n">pre_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221123235913097.png" alt="image-20221123235913097"  />
</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.5.png" alt="Figure_3.5"  />
</p>
<h4 id="算法评价">算法评价<a hidden class="anchor" aria-hidden="true" href="#算法评价">#</a></h4>
<p><strong>优点：</strong></p>
<p>1）算法简单，理论成熟，既可以用来做分类也可以用来做回归。</p>
<p>2）可用于非线性分类。</p>
<p>3）没有明显的训练过程，而是在程序开始运行时，把数据集加载到内存后，不需要进行训练，直接进行预测，所以训练时间复杂度为0。</p>
<p>4）由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属的类别，因此对于类域的交叉或重叠较多的待分类样本集来说，KNN方法较其他方法更为适合。</p>
<p>5）该算法比较适用于样本容量比较大的类域的自动分类，而那些样本容量比较小的类域采用这种算法比较容易产生误分类情况。</p>
<p><strong>缺点：</strong></p>
<p>1）需要算每个测试点与训练集的距离，当训练集较大时，计算量相当大，时间复杂度高，特别是特征数量比较大的时候。</p>
<p>2）需要大量的内存，空间复杂度高。</p>
<p>3）样本不平衡问题（即有些类别的样本数量很多，而其它样本的数量很少），对稀有类别的预测准确度低。</p>
<p>4）是lazy learning方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。</p>
<h2 id="dbscan算法">DBSCAN算法<a hidden class="anchor" aria-hidden="true" href="#dbscan算法">#</a></h2>
<h3 id="算法原理-1">算法原理<a hidden class="anchor" aria-hidden="true" href="#算法原理-1">#</a></h3>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法）是一种基于密度的空间聚类算法。该算法将具有足够密度的区域划分为簇，并在具有噪声的空间数据库中发现任意形状的簇，它将簇定义为密度相连的点的最大集合。</p>
<p>这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。</p>
<h4 id="基本概念-1">基本概念<a hidden class="anchor" aria-hidden="true" href="#基本概念-1">#</a></h4>
<p>若给定数据集$D={x_1,x_2,&hellip;,x_i}$</p>
<p>$\epsilon$-邻域: 对 $\boldsymbol{x}_j \in D$, 其 $\epsilon$-邻域包含样本集 $D$ 中与 $\boldsymbol{x}<em>j$ 的距离不大于 $\epsilon$ 的样本,即 $N</em>\epsilon\left(\boldsymbol{x}_j\right)=\left{\boldsymbol{x}_i \in D \mid \operatorname{dist}\left(\boldsymbol{x}_i, \boldsymbol{x}_j\right) \leqslant \epsilon\right}$;</p>
<p>1、核心对象：若$x_j$的$\epsilon$邻域内有超过MinPts个样本，$x_j$是一个核心对象；</p>
<p>2、密度直达：$x_i$是核心对象，$x_j$位于$x_i$的$\epsilon$邻域内，则称$x_j$由$x_i$密度直达；</p>
<p>3、密度可达：对 $\boldsymbol{x}_i$ 与 $\boldsymbol{x}_j$, 若存在样本序列 $\boldsymbol{p}_1, \boldsymbol{p}_2, \ldots, \boldsymbol{p}_n$, 其中 $\boldsymbol{p}_1=\boldsymbol{x}_i, \boldsymbol{p}_n \doteq \boldsymbol{x}<em>j$ 且 $\boldsymbol{p}</em>{i+1}$ 由 $\boldsymbol{p}_i$ 密度直达, 则称 $\boldsymbol{x}_j$ 由 $\boldsymbol{x}_i$ 密度可达;</p>
<p>4、密度相连：对 $\boldsymbol{x}_i$ 与 $\boldsymbol{x}_j$, 若存在 $\boldsymbol{x}_k$ 使得 $\boldsymbol{x}_i$ 与 $\boldsymbol{x}_j$ 均由 $\boldsymbol{x}_k$ 密度可达, 则称 $\boldsymbol{x}_i$ 与 $\boldsymbol{x}_j$ 密度相连.</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221122231950589.png" alt="image-20221122231950589"  />
</p>
<ul>
<li>核心点 (Core point)。若样本 $x_i$ 的 $\varepsilon$ 邻域内至少包含了MinPts个样本, 即 $N_{\varepsilon}\left(X_i\right) \geq$ MinPts，则称样本点 $x_i$ 为核心点。</li>
<li>边界点 (Border point)。若样本 $x_i$ 的 $\varepsilon$ 邻域内包含的样本数目小于MinPts，但是它在其他核心点的邻域内，则称样本点 $x_i$ 为边界点。</li>
<li>噪音点 (Noise)。既不是核心点也不是边界点的点</li>
</ul>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221122232258204.png" alt="image-20221122232258204"  />
</p>
<p><strong>簇的定义</strong></p>
<p>DBSCAN 算法对簇的定义很简单，由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个簇。</p>
<h4 id="算法流程">算法流程<a hidden class="anchor" aria-hidden="true" href="#算法流程">#</a></h4>
<p>输入: 样本集 $\mathrm{D}=\left(x_1, x_2, \ldots, x_m\right)$ ，邻域参数 $(\epsilon$, MinPts $)$ ，样本距离度量方式
输出: 簇划分C.</p>
<ol>
<li>初始化核心对象集合 $\Omega=\emptyset ，$ 初始化聚类簇数 $\mathrm{k}=0$ ，初始化末访问样本集合 $\Gamma=\mathrm{D}$ ，簇划分 $\mathrm{C}=\emptyset$</li>
<li>对于 $j=1,2, \ldots m$ ，按下面的步㝡找出所有的核心对象:
a) 通过距离庹量方式，找到样本 $x_j$ 的 $\epsilon$-邻域子样本集 $N_\epsilon\left(x_j\right)$
b) 如果子样本集样本个数满足 $\left|N_\epsilon\left(x_j\right)\right| \geq \operatorname{MinPts}$ ， 将样本 $x_j$ 加入核心对象样本集合: $\Omega=\Omega \cup\left{x_j\right}$</li>
<li>如果核心对象集合 $\Omega=\emptyset$ ，则算法结束，否则转入步骤4.</li>
<li>在核心对象集合 $\Omega$ 中，随机选择一个核心对象 $O$ ，初始化当前笶核心对象队列 $\Omega_{c u r}={o}$ ，初始化类别序号 $\mathrm{k}=\mathrm{k}+1$ ，初始化当前簇样本集合 $C_k={o}$ ，更新末访问样本集合 $\Gamma=\Gamma-{o}$</li>
<li>如果当前笶核心对象队列 $\Omega_{c u r}=\emptyset$, 则当前聚类箷 $C_k$ 生成完毕，更新簇划分 $\mathrm{C}=\left{C_1, C_2, \ldots, C_k\right}$ ，更新核心对象集合 $\Omega=\Omega-C_k$ ，转入步 骤3。否则更新核心对象集合 $\Omega=\Omega-C_k$ 。</li>
<li>在当前簇核心对象队列 $\Omega_{c u r}$ 中取出一个核心对象 $o^{\prime}$, 通过邻域距离阈值 $\epsilon$ 找出所有的 $\epsilon$-邻域子样本集 $N_\epsilon\left(o^{\prime}\right)$ ，令 $\Delta=N_\epsilon\left(o^{\prime}\right) \cap \Gamma$ ，更新当前簇样本 集合 $C_k=C_k \cup \Delta$ ，更新末访问样本集合 $\Gamma=\Gamma-\Delta$ ，更新 $\Omega_{c u r}=\Omega_{c u r} \cup(\Delta \cap \Omega)-o^{\prime}$ ，转入步噔5.
输出结果为: 簇划分 $\mathrm{C}=\left{C_1, C_2, \ldots, C_k\right}$</li>
</ol>
<h3 id="代码实现-1">代码实现<a hidden class="anchor" aria-hidden="true" href="#代码实现-1">#</a></h3>
<h4 id="加载数据">加载数据<a hidden class="anchor" aria-hidden="true" href="#加载数据">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Iris Dataset&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_2.1.png" alt="Figure_2.1"  />
</p>
<h4 id="计算距离">计算距离<a hidden class="anchor" aria-hidden="true" href="#计算距离">#</a></h4>
<p>这里的距离度量是用的二范数或者欧几里得范数</p>
<p>The $L_2$-norm (or 2-norm, or Euclidean norm)
$$
|\boldsymbol{x}|<em>2=\sqrt{\sum</em>{i=1}^n x_i^2}
$$
然后下方这个函数就主要负责距离的查询</p>
<p>选择每次的首个核心点，并传入参数<code>epsilon</code>和<code>min_points</code>进行首次迭代</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">point_id</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Find all points within eps distance of point</span>
</span></span><span class="line"><span class="cl">    <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="n">point_id</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">neighbors</span>
</span></span></code></pre></div><h4 id="寻找近邻点">寻找近邻点<a hidden class="anchor" aria-hidden="true" href="#寻找近邻点">#</a></h4>
<p>首个核心点迭代完成后，对它进行移动，直到出现不满足<code>阈值</code>条件的样本点为止</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">expandCluster</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span><span class="n">cluster_id</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 返回所有近邻点</span>
</span></span><span class="line"><span class="cl">    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># print(neighbors)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果点的密度小于min_points，则为噪声点</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">min_points</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果点的密度大于min_points，则为核心点</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为核心点赋新簇标签</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">neighbor</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span><span class="c1">#为eps半径内的点赋新簇标签</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 遍历所有近邻点</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#对近邻点进行扩展</span>
</span></span><span class="line"><span class="cl">            <span class="n">current_point</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="c1">#取出第一个点</span>
</span></span><span class="line"><span class="cl">            <span class="n">query_results</span> <span class="o">=</span> <span class="n">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">current_point</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span><span class="c1">#找出该点的所有近邻点</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">query_results</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">min_points</span><span class="p">:</span><span class="c1">#如果该点的密度大于min_points，则为核心点</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">query_results</span><span class="p">)):</span><span class="c1">#遍历所有近邻点</span>
</span></span><span class="line"><span class="cl">                    <span class="n">result_point</span> <span class="o">=</span> <span class="n">query_results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="c1">#如果该点为噪声点，则赋新簇标签</span>
</span></span><span class="line"><span class="cl">                        <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#如果该点未访问过，则赋新簇标签，并加入neighbors</span>
</span></span><span class="line"><span class="cl">                        <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">                        <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">neighbors</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">current_point</span><span class="p">)</span><span class="c1">#删除当前点</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">True</span>
</span></span></code></pre></div><h4 id="dbscan">dbscan<a hidden class="anchor" aria-hidden="true" href="#dbscan">#</a></h4>
<p><em>初始化一个空的分类列表，对其中每个未分类点进行调用上述函数</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dbscan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">cluster_id</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="c1">#初始化一个空的分类列表</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">point_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#如果该点未访问过</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">expandCluster</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span><span class="n">cluster_id</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1">#可变类型：类似 c++ 的引用传递，如 列表，字典，类等</span>
</span></span><span class="line"><span class="cl">                <span class="n">cluster_id</span> <span class="o">=</span> <span class="n">cluster_id</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">cluster_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">label</span>
</span></span></code></pre></div><h4 id="可视化模型结果">可视化模型结果<a hidden class="anchor" aria-hidden="true" href="#可视化模型结果">#</a></h4>
<p>按照前两个参数进行画图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;DBSCAN&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">label_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sepal length&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sepal width&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#图例</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_2.2.png" alt="Figure_2.2"  />
</p>
<h4 id="评估k-means模型">评估K-means模型<a hidden class="anchor" aria-hidden="true" href="#评估k-means模型">#</a></h4>
<p><code>print(&quot;轮廓系数为:&quot;,metrics.silhouette_score(data, label_pred, metric='euclidean'))</code></p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221123170922355.png" alt="image-20221123170922355"  />
</p>
<p>与sklearn的对比</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">DBSCAN</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">iris</span><span class="o">=</span><span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">label</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl"><span class="n">db</span> <span class="o">=</span> <span class="n">DBSCAN</span><span class="p">(</span><span class="n">eps</span><span class="o">=</span><span class="mf">0.46</span><span class="p">,</span> <span class="n">min_samples</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">skl_labels</span> <span class="o">=</span> <span class="n">db</span><span class="o">.</span><span class="n">labels_</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;dbscan sklearn&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">skl_labels</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_2.3.png" alt="Figure_2.3"  />
</p>
<h4 id="完整代码">完整代码<a hidden class="anchor" aria-hidden="true" href="#完整代码">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">point_id</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Find all points within eps distance of point</span>
</span></span><span class="line"><span class="cl">    <span class="n">neighbors</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">data</span><span class="p">[</span><span class="n">point_id</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">eps</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">neighbors</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">expandCluster</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span><span class="n">cluster_id</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 返回所有近邻点</span>
</span></span><span class="line"><span class="cl">    <span class="n">neighbors</span> <span class="o">=</span> <span class="n">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># print(neighbors)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果点的密度小于min_points，则为噪声点</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">min_points</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">False</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 如果点的密度大于min_points，则为核心点</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 为核心点赋新簇标签</span>
</span></span><span class="line"><span class="cl">        <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">neighbor</span> <span class="ow">in</span> <span class="n">neighbors</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">label</span><span class="p">[</span><span class="n">neighbor</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span><span class="c1">#为eps半径内的点赋新簇标签</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 遍历所有近邻点</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">neighbors</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#对近邻点进行扩展</span>
</span></span><span class="line"><span class="cl">            <span class="n">current_point</span> <span class="o">=</span> <span class="n">neighbors</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="c1">#取出第一个点</span>
</span></span><span class="line"><span class="cl">            <span class="n">query_results</span> <span class="o">=</span> <span class="n">region_query</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">current_point</span><span class="p">,</span> <span class="n">eps</span><span class="p">)</span><span class="c1">#找出该点的所有近邻点</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">query_results</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">min_points</span><span class="p">:</span><span class="c1">#如果该点的密度大于min_points，则为核心点</span>
</span></span><span class="line"><span class="cl">                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">query_results</span><span class="p">)):</span><span class="c1">#遍历所有近邻点</span>
</span></span><span class="line"><span class="cl">                    <span class="n">result_point</span> <span class="o">=</span> <span class="n">query_results</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                    <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="c1">#如果该点为噪声点，则赋新簇标签</span>
</span></span><span class="line"><span class="cl">                        <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">                    <span class="k">elif</span> <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#如果该点未访问过，则赋新簇标签，并加入neighbors</span>
</span></span><span class="line"><span class="cl">                        <span class="n">label</span><span class="p">[</span><span class="n">result_point</span><span class="p">]</span> <span class="o">=</span> <span class="n">cluster_id</span>
</span></span><span class="line"><span class="cl">                        <span class="n">neighbors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">result_point</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="n">neighbors</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">current_point</span><span class="p">)</span><span class="c1">#删除当前点</span>
</span></span><span class="line"><span class="cl">        <span class="k">return</span> <span class="kc">True</span>
</span></span><span class="line"><span class="cl"><span class="c1">#初始化一个空的分类列表，对其中每个未分类点进行调用上述函数</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">dbscan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">cluster_id</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="c1">#初始化一个空的分类列表</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">point_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">label</span><span class="p">[</span><span class="n">point_id</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span><span class="c1">#如果该点未访问过</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">expandCluster</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">label</span><span class="p">,</span><span class="n">point_id</span><span class="p">,</span><span class="n">cluster_id</span><span class="p">,</span><span class="n">eps</span><span class="p">,</span><span class="n">min_points</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">                <span class="c1">#可变类型：类似 c++ 的引用传递，如 列表，字典，类等</span>
</span></span><span class="line"><span class="cl">                <span class="n">cluster_id</span> <span class="o">=</span> <span class="n">cluster_id</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">cluster_id</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">label</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">showCluster</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">n_clusters_</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">label</span><span class="p">))</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">in</span> <span class="n">label</span> <span class="k">else</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">n_clusters_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># Black removed and is used for noise instead.</span>
</span></span><span class="line"><span class="cl">    <span class="n">unique_labels</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">(</span><span class="n">each</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">              <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">))]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">col</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">unique_labels</span><span class="p">,</span> <span class="n">colors</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># Black used for noise.</span>
</span></span><span class="line"><span class="cl">            <span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">class_member_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">label</span> <span class="o">==</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">xy</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">class_member_mask</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xy</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">xy</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">markerfacecolor</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">col</span><span class="p">),</span>
</span></span><span class="line"><span class="cl">                 <span class="n">markeredgecolor</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Estimated number of clusters: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">n_clusters_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl">    <span class="n">label</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Iris Dataset&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">label_pred</span> <span class="o">=</span> <span class="n">dbscan</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="mf">0.46</span><span class="p">,</span><span class="mi">11</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="n">label_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># print(label_pred)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;DBSCAN&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">label_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#图例</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><h4 id="算法评价-1">算法评价<a hidden class="anchor" aria-hidden="true" href="#算法评价-1">#</a></h4>
<p><strong>优点：</strong></p>
<ol>
<li>不需要设置k值</li>
<li>可以发现任意形状的蔟</li>
<li>可以聚类的同时发现噪音点，即对噪音不敏感</li>
<li>对样本输入顺序不敢兴趣</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>高维数据效果不理想</li>
<li>调参复杂，eps和Minpiont参数不好设置，无法预估。</li>
</ol>
<h2 id="k-means算法">K-means算法<a hidden class="anchor" aria-hidden="true" href="#k-means算法">#</a></h2>
<h3 id="算法原理-2">算法原理<a hidden class="anchor" aria-hidden="true" href="#算法原理-2">#</a></h3>
<blockquote>
<p>K-means算法是一种聚类算法，所谓聚类，即根据相似性原则，将具有较高相似度的数据对象划分至同一类簇，将具有较高相异度的数据对象划分至不同类簇。聚类与分类最大的区别在于，聚类过程为无监督过程，即待处理数据对象没有任何先验知识，而分类过程为有监督过程，即存在有先验知识的训练数据集。</p>
<p>k-means算法中的k代表类簇个数，means代表类簇内数据对象的均值（这种均值是一种对类簇中心的描述），因此，k-means算法又称为k-均值算法。k-means算法是一种基于划分的聚类算法，以距离作为数据对象间相似性度量的标准，即数据对象间的距离越小，则它们的相似性越高，则它们越有可能在同一个类簇。数据对象间距离的计算有很多种，k-means算法通常采用欧氏距离来计算数据对象间的距离。</p>
</blockquote>
<h4 id="算法流程-1">算法流程<a hidden class="anchor" aria-hidden="true" href="#算法流程-1">#</a></h4>
<p>算法接受一个未标记的数据集，然后将数据聚类成不同的组。假设将数据分成k个组，方法为：</p>
<ol>
<li>选择初始化的 $\mathrm{k}$ 个样本作为初始聚类中心 $a=a_1, a_2, \ldots a_k$ ；</li>
<li>针对数据集中每个样本 $x_i$ 计算它到 $\mathrm{k}$ 个聚类中心的距离并将其分到距离最小的聚类中心所对应 的类中；</li>
<li>针对每个类别 $a_j$ ，重新计算它的聚类中心 $a_j=\frac{1}{\left|c_i\right|} \sum_{x \in c_i} x$ (即属于该类的所有样本的质
心）；</li>
<li>重复上面 23 两步操作，直到达到某个中止条件 (迭代次数、最小误差变化等)。</li>
</ol>
<p>吴恩达视频的中的伪代码为</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-shell" data-lang="shell"><span class="line"><span class="cl">repeat <span class="o">{</span>
</span></span><span class="line"><span class="cl">  <span class="k">for</span> <span class="nv">i</span><span class="o">=</span> to m
</span></span><span class="line"><span class="cl">  <span class="c1">#  计算每个样例属于的类</span>
</span></span><span class="line"><span class="cl">  c<span class="o">(</span>i<span class="o">)</span> :<span class="o">=</span> index <span class="o">(</span>from <span class="m">1</span> to K<span class="o">)</span>  of cluster centroid closest to x<span class="o">(</span>i<span class="o">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"> <span class="k">for</span> <span class="nv">k</span> <span class="o">=</span> <span class="m">1</span> to K
</span></span><span class="line"><span class="cl">  <span class="c1"># 聚类中心的移动，重新计算该类的质心</span>
</span></span><span class="line"><span class="cl"> u<span class="o">(</span>k<span class="o">)</span> :<span class="o">=</span> average <span class="o">(</span>mean<span class="o">)</span> of points assigned to cluster K
</span></span><span class="line"><span class="cl"><span class="o">}</span>
</span></span></code></pre></div><h4 id="优化目标">优化目标<a hidden class="anchor" aria-hidden="true" href="#优化目标">#</a></h4>
<p>这里的终止条件，我们用K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此K-均值的代价函数（畸变函数Distortion function） ：
$$
\begin{equation}
J\left(c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K\right)=\frac{1}{m} \sum_{i=1}^m\left|X^{(i)}-\mu_{c^{(i)}}\right|^2
\end{equation}
$$
其中$\mu$代表与$x_i$最近的聚类中心点,$c^i$代表族类</p>
<p>优化目标就是找出使得代价函数最小的c和μ，即
$$
\begin{aligned}
J\left(\underline{c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K}\right)=\frac{1}{m} \sum_{i=1}^m\left|x^{(i)}-\mu_{c^{(i)}}\right|^2\
\min _{\substack{c^{(1)}, \ldots, c^{(m)}\
\mu_1, \ldots, \mu_K}} J\left(c^{(1)}, \ldots, c^{(m)}, \mu_1, \ldots, \mu_K\right) \
&amp;
\end{aligned}
$$</p>
<h3 id="代码实现-2">代码实现<a hidden class="anchor" aria-hidden="true" href="#代码实现-2">#</a></h3>
<h4 id="加载数据-1">加载数据<a hidden class="anchor" aria-hidden="true" href="#加载数据-1">#</a></h4>
<p>我们用Python的scikit-learn库中自带的鸢尾花数据集，并使用使用datasets.load_iris()载入，并用开头的两个维度进行画图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#加载数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#划分训练集和测试集</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">showData</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#显示数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">lable_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sepal length&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sepal width&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_1.png" alt="Figure_1"  />
</p>
<h4 id="数据处理-1">数据处理<a hidden class="anchor" aria-hidden="true" href="#数据处理-1">#</a></h4>
<p>不同特征之间往往具有不同的量纲，由此所造成的数值间的差异可能很大，在涉及空间距离计算或梯度下降法等情况的时候不对其进行处理会影响到数据分析结果的准确性。为了消除特征之间的量纲和取值范围差异可能会造成的影响，需对数据进行标准化处理，也可以称为规范化处理。
在这里我们对数据集进行标准差标准化处理</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl">  <span class="n">MMS</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">  <span class="n">X_data</span> <span class="o">=</span> <span class="n">MMS</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="计算距离-1">计算距离<a hidden class="anchor" aria-hidden="true" href="#计算距离-1">#</a></h4>
<p>初始化质心，我们选取k个样本作为初始的类的中心</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">initCentroids</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#初始化质心</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#随机选取k个样本作为初始质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">numSamples</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">numSamples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">centroids</span>
</span></span></code></pre></div><p>计算每个样本到类中心的距离，并选取其中欧拉距离最小的，并打上属于某个类的标记</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">OulerDistance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#欧拉距离</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算两个向量的欧拉距离 :math:`d = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}`</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#vec1-vec2表示两个向量的对应元素的差</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vec1</span> <span class="o">-</span> <span class="n">vec2</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">minDistance</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">centroids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算每个样本到质心的距离</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回每个样本所属的簇</span>
</span></span><span class="line"><span class="cl">    <span class="n">numSamples</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">clusterDict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span><span class="o">=</span><span class="n">centroids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">flower</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">vec1</span><span class="o">=</span><span class="n">flower</span>
</span></span><span class="line"><span class="cl">        <span class="n">flag</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="n">minDist</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span><span class="c1">#无穷大</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec2</span><span class="o">=</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">distance</span> <span class="o">=</span> <span class="n">OulerDistance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distance</span> <span class="o">&lt;</span> <span class="n">minDist</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">minDist</span> <span class="o">=</span> <span class="n">distance</span>
</span></span><span class="line"><span class="cl">                <span class="n">flag</span> <span class="o">=</span> <span class="n">i</span><span class="c1">#标记为第i个簇</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">flag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">clusterDict</span><span class="p">[</span><span class="n">flag</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">clusterDict</span><span class="p">[</span><span class="n">flag</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flower</span><span class="p">)</span><span class="c1">#将该样本加入到第i个簇中</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">clusterDict</span><span class="c1">#返回每个样本所属的簇</span>
</span></span></code></pre></div><p>求两个向量的欧拉距离:
$$
d = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}
$$
获取更新新的聚类中心</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">getCentroids</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算新的质心</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回新的质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">cluster</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">centroids</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算簇内误差平方和</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回簇内误差平方和</span>
</span></span><span class="line"><span class="cl">    <span class="n">variation</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">variation</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">cluster</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">variation</span>
</span></span></code></pre></div><p>针对每个类别 $a_j$,重新计算它的聚类中心 $a_j=\frac{1}{\left|c_i\right|} \sum_{x \in c_i} x$ (即属于该类的所有样本的质心）；</p>
<h4 id="寻找最小mean">寻找最小mean<a hidden class="anchor" aria-hidden="true" href="#寻找最小mean">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"> <span class="n">new_variation</span> <span class="o">=</span> <span class="n">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#显示图形</span>
</span></span><span class="line"><span class="cl">    <span class="n">old_variation</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_variation</span> <span class="o">-</span> <span class="n">old_variation</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.0001</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_variation</span> <span class="o">=</span> <span class="n">new_variation</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span> <span class="o">=</span> <span class="n">getCentroids</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">clusterDict</span> <span class="o">=</span> <span class="n">minDistance</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_variation</span> <span class="o">=</span> <span class="n">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="可视化模型">可视化模型<a hidden class="anchor" aria-hidden="true" href="#可视化模型">#</a></h4>
<p>根据前两个维度画出散点图</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">showCluster</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">clusterDict</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#获取聚类标签</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#绘制样本点</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label2&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sepal length&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sepal width&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#绘制质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centroids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;centroids&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;the result of K-means&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_2.png" alt="Figure_2"  />
</p>
<h4 id="评估k-means模型-1">评估K-means模型<a hidden class="anchor" aria-hidden="true" href="#评估k-means模型-1">#</a></h4>
<p><code>print('轮廓系数为：', silhouette_score(X_data, lable_data, metric='euclidean'))</code></p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/image-20221122203932608.png" alt="image-20221122203932608"  />
</p>
<p>为了作对比这里直接调用sklearn库进行模拟，方便我们分析，对比</p>
<p>在这里我们获取轮廓系数score是所有样本的轮廓系数均值，如果要获取每个样本的轮廓系数应当使用silhouette_samples。这里是针对超参数k(n_cluster)，所以采用轮廓系数均值进行评估。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">iris_data</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">x</span> <span class="o">=</span> <span class="n">iris_data</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris_data</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl"><span class="n">MMS</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="n">MMS</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#构建KMeans模型训练数据</span>
</span></span><span class="line"><span class="cl"><span class="n">cluster</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1">#获取聚类结果</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">labels_</span>
</span></span><span class="line"><span class="cl"><span class="c1">#获取聚类中心</span>
</span></span><span class="line"><span class="cl"><span class="n">centers</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">cluster_centers_</span>
</span></span><span class="line"><span class="cl"><span class="c1">#获取聚类的内部误差平方和</span>
</span></span><span class="line"><span class="cl"><span class="n">inertia</span> <span class="o">=</span> <span class="n">cluster</span><span class="o">.</span><span class="n">inertia_</span>
</span></span><span class="line"><span class="cl"><span class="c1">#计算轮廓系数</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;轮廓系数为：&#39;</span><span class="p">,</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="c1">#显示聚类结果</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centers</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centers</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">silhouetteScore</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">15</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 构建并训练模型</span>
</span></span><span class="line"><span class="cl">    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">i</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">score</span> <span class="o">=</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">silhouetteScore</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span><span class="n">silhouetteScore</span><span class="p">,</span><span class="n">linewidth</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span><span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;the number of clusters&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;silhouette coefficient&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_3.png" alt="Figure_3"  />
</p>
<p>可以看到聚类数目为2、3和4、5的时候，图形的畸变程度最大。本身数据集就是关于3种鸢尾花的，侧面说明了聚类为3的时候效果较好，且用库的和实现的效果一致</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/Figure_4.png" alt="Figure_4"  />
</p>
<h4 id="完整代码-1">完整代码<a hidden class="anchor" aria-hidden="true" href="#完整代码-1">#</a></h4>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">showData</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#显示数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">lable_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sepal length&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sepal width&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;the original data&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">showCluster</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">clusterDict</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#获取聚类标签</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="n">labels2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#绘制样本点</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels0</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels0</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label0&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels1</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels1</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label1&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">labels2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">labels2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;label2&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;sepal length&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;sepal width&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#绘制质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">centroids</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">centroids</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;+&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;centroids&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;the result of K-means&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">OulerDistance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#欧拉距离</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算两个向量的欧拉距离 :math:`d = \sqrt{\sum_{i=1}^n (x_i - y_i)^2}`</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#vec1-vec2表示两个向量的对应元素的差</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">vec1</span> <span class="o">-</span> <span class="n">vec2</span><span class="p">)))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">initCentroids</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#初始化质心</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#随机选取k个样本作为初始质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">numSamples</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="p">,</span> <span class="n">dim</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">index</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">numSamples</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">index</span><span class="p">,</span> <span class="p">:]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">centroids</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">minDistance</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">centroids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算每个样本到质心的距离</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回每个样本所属的簇</span>
</span></span><span class="line"><span class="cl">    <span class="n">numSamples</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="n">clusterDict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span><span class="o">=</span><span class="n">centroids</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">flower</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">vec1</span><span class="o">=</span><span class="n">flower</span>
</span></span><span class="line"><span class="cl">        <span class="n">flag</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
</span></span><span class="line"><span class="cl">        <span class="n">minDist</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span><span class="c1">#无穷大</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">            <span class="n">vec2</span><span class="o">=</span><span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">distance</span> <span class="o">=</span> <span class="n">OulerDistance</span><span class="p">(</span><span class="n">vec1</span><span class="p">,</span> <span class="n">vec2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">            <span class="k">if</span> <span class="n">distance</span> <span class="o">&lt;</span> <span class="n">minDist</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">minDist</span> <span class="o">=</span> <span class="n">distance</span>
</span></span><span class="line"><span class="cl">                <span class="n">flag</span> <span class="o">=</span> <span class="n">i</span><span class="c1">#标记为第i个簇</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">flag</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">            <span class="n">clusterDict</span><span class="p">[</span><span class="n">flag</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">clusterDict</span><span class="p">[</span><span class="n">flag</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">flower</span><span class="p">)</span><span class="c1">#将该样本加入到第i个簇中</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">clusterDict</span><span class="c1">#返回每个样本所属的簇</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">getCentroids</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算新的质心</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回新的质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])))</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">cluster</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cluster</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cluster</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">centroids</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算簇内误差平方和</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#返回簇内误差平方和</span>
</span></span><span class="line"><span class="cl">    <span class="n">variation</span> <span class="o">=</span> <span class="mf">0.0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">cluster</span> <span class="ow">in</span> <span class="n">clusterDict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">        <span class="n">variation</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">cluster</span> <span class="o">-</span> <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]))</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">variation</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">Kmeans</span><span class="p">():</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#加载数据</span>
</span></span><span class="line"><span class="cl">    <span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#划分训练集和测试集</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># </span>
</span></span><span class="line"><span class="cl">    <span class="n">showData</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#数据归一化</span>
</span></span><span class="line"><span class="cl">    <span class="n">MMS</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">X_data</span> <span class="o">=</span> <span class="n">MMS</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#初始化质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">initCentroids</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算每个样本到质心的距离</span>
</span></span><span class="line"><span class="cl">    <span class="n">clusterDict</span> <span class="o">=</span> <span class="n">minDistance</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算新的质心</span>
</span></span><span class="line"><span class="cl">    <span class="n">centroids</span> <span class="o">=</span> <span class="n">getCentroids</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算簇内误差平方和</span>
</span></span><span class="line"><span class="cl">    <span class="n">new_variation</span> <span class="o">=</span> <span class="n">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#显示图形</span>
</span></span><span class="line"><span class="cl">    <span class="n">old_variation</span> <span class="o">=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">    <span class="k">while</span> <span class="nb">abs</span><span class="p">(</span><span class="n">new_variation</span> <span class="o">-</span> <span class="n">old_variation</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.0001</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">old_variation</span> <span class="o">=</span> <span class="n">new_variation</span>
</span></span><span class="line"><span class="cl">        <span class="n">centroids</span> <span class="o">=</span> <span class="n">getCentroids</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">clusterDict</span> <span class="o">=</span> <span class="n">minDistance</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">new_variation</span> <span class="o">=</span> <span class="n">getVaration</span><span class="p">(</span><span class="n">clusterDict</span><span class="p">,</span> <span class="n">centroids</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="n">showCluster</span><span class="p">(</span><span class="n">centroids</span><span class="p">,</span> <span class="n">clusterDict</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1">#计算轮廓系数</span>
</span></span><span class="line"><span class="cl">    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;轮廓系数为：&#39;</span><span class="p">,</span> <span class="n">silhouette_score</span><span class="p">(</span><span class="n">X_data</span><span class="p">,</span> <span class="n">lable_data</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">    <span class="n">Kmeans</span><span class="p">()</span>
</span></span></code></pre></div><h4 id="算法评价-2">算法评价<a hidden class="anchor" aria-hidden="true" href="#算法评价-2">#</a></h4>
<p>优点：</p>
<p>1.是解决聚类问题的一种经典算法，简单、快速</p>
<p>2.对处理大数据集，该算法保持可伸缩性和高效率</p>
<p>3.当结果簇是密集的，它的效果较好</p>
<p>缺点</p>
<p>1.在簇的平均值可被定义的情况下才能使用，可能不适用于某些应用</p>
<p>2.必须事先给出k（要生成的簇的数目），而且对初值敏感，对于不同的初始值，可能会导致不同结果。</p>
<p>3.不适合于发现非凸形状的簇或者大小差别很大的簇</p>
<p>4.对躁声和孤立点数据敏感</p>
<h2 id="参考文献">参考文献<a hidden class="anchor" aria-hidden="true" href="#参考文献">#</a></h2>
<p><a href="https://www.coursera.org/learn/machine-learning?spm=a2c6h.12873639.article-detail.8.7cac2622OtclGn">Supervised Machine Learning: Regression and Classification</a></p>
<p><a href="https://blog.51cto.com/u_15749390/5570555">https://blog.51cto.com/u_15749390/5570555</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/53084915">https://zhuanlan.zhihu.com/p/53084915</a></p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://niceasiv.cn/posts/pe_dll/">
    <span class="title">« 上一页</span>
    <br>
    <span>通过修改 PE 装载 DLL 实验</span>
  </a>
  <a class="next" href="https://niceasiv.cn/posts/calling_convention/">
    <span class="title">下一页 »</span>
    <br>
    <span>调用惯例</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on twitter"
       href="https://twitter.com/intent/tweet/?text=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f&amp;hashtags=MachineLearning%2cKNN%2cDBscan%2cK-means">
    <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on linkedin"
       href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f&amp;title=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0&amp;summary=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0&amp;source=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on reddit"
       href="https://reddit.com/submit?url=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f&title=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on facebook"
       href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on whatsapp"
       href="https://api.whatsapp.com/send?text=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0%20-%20https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KNN&amp;DBscan&amp;K-means实现 on telegram"
       href="https://telegram.me/share/url?text=KNN%26DBscan%26K-means%e5%ae%9e%e7%8e%b0&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2fcluster%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

        </footer>
    </div>
</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2021-2023 Asiv&#39;s Blog
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;">陕ICP备2021000000号</a>&nbsp;
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Asiv's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Asiv's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script></body>

</html>
