<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>

<title>强化学习笔记 | Asiv&#39;s Blog</title>
<meta name="keywords" content="Machine Learning, Reinforcement learning">
<meta name="description" content="基础 马尔科夫决策(MDP) 在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。
如果已知历史的状态信息即$(S_1,&hellip;,S_t)$,那么下一个时刻状态为$S_{t&#43;1}$的概率为$P(S_{t&#43;1}\mid S_1,&hellip;,S_t)$
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质
$$ P(S_{t&#43;1}\mid S_t)=P(S_{t&#43;1}\mid S_1,&hellip;,S_t) $$ 也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。
注意：
虽然$t&#43;1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。
Markov process 通常用两元组$&lt;S,P&gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵
矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。
给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。
Markov reward process(MRP) 一个马尔科夫奖励过程由$\langle \mathcal{S},\mathcal{P},r,\gamma \rangle$
$\mathcal{S}$ 是有限状态的集合。 $\mathcal{P}$ 是状态转移矩阵。 $r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。 $\gamma$ 是折扣因子,$\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近0的$\gamma$ 更考虑短期奖励。 奖励函数的本质:向智能体传达目标(MDP) 强化学习的标准交互过程如下：每个时刻，智能体根据根据其 策略(policy)，在当前所处 状态(state) 选择一个 动作(action)，环境(environment) 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 奖励信号 (reward)，这通常是一个数值，奖励的折扣累加和称为 收益/回报 (return)，是智能体在动作选择过程中想要最大化的目标
“奖励 &amp; 收益” 其实是智能体目标的一种形式化、数值化的表征。可以把这种想法非正式地表述为 “收益假设” 收益是通过奖励信号计算的，而奖励函数是我们提供的，奖励函数起到了人与算法沟通的桥梁作用 需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标 回报 在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报（Return）
假设设置$\gamma =0.5$
$$ G_t=R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t&#43;k} $$ 假设路径为1,2,3,6
import numpy as np np.random.seed(0) #概率转移矩阵 P=[ [0.9,0.1,0.0,0.0,0.0,0.0], [0.5,0.0,0.5,0.0,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,0.4], [0.0,0.0,0.0,0.0,0.3,0.7], [0.0,0.2,0.3,0.5,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,1.0], ] P=np.array(P) rewards=[-1,-2,-2,10,1,0] gamma=0.5 def compute_chain_reward(start_index,chain,gamma): reward=0 for i in range(len(chain)): reward=gamma*reward&#43;rewards[chain[i]-1] return reward chain=[1,2,3,6] start_index=0 reward=compute_chain_reward(start_index,chain,gamma) print(reward) 状态价值函数 一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-&gt;价值
即其状态价值函数$V_{\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即
$V(s)=\mathbb{E}\left[G_t \mid S_t=s\right]$
展开为: $$ \begin{align} V(s) &amp;=\mathbb{E}[G_t \mid S_t=s] \ &amp;=\mathbb{E}[R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;&hellip;\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma(R_{t&#43;1}&#43;\gamma R_{t&#43;2})&#43;&hellip; \mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma G_{t&#43;1}\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma V(S_{t&#43;1})\mid S_t=s]\ \end{align} $$ 一方面,即时奖励的期望正是奖励函数的输出，即：
$\mathbb{E}\left[R_t \mid S_t=s\right]=r(s)$
另一方面，等式中剩余部分 $\mathbb{E}\left[\gamma V\left(S_{t&#43;1}\right) \mid S_t=s\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到 $$ V(s)=r(s)&#43;\gamma\sum_{s^{\prime} \in S }{p(s^{\prime} \mid s)V(s^\prime)} $$ 上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。">
<meta name="author" content="Asiv">
<link rel="canonical" href="https://niceasiv.cn/posts/reinforment_learning/">
<meta name="google-site-verification" content="NiceAsiv.cn">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.js" onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://niceasiv.cn/img/logo.jpg">
<link rel="icon" type="image/png" sizes="16x16" href="https://niceasiv.cn/img/logo.jpg">
<link rel="icon" type="image/png" sizes="32x32" href="https://niceasiv.cn/img/logo.jpg">
<link rel="apple-touch-icon" href="https://niceasiv.cn/img/logo.jpg">
<link rel="mask-icon" href="https://niceasiv.cn/img/logo.jpg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.3/dist/jquery.min.js"></script>



<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = ""; 
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
<meta property="og:title" content="强化学习笔记" />
<meta property="og:description" content="基础 马尔科夫决策(MDP) 在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。
如果已知历史的状态信息即$(S_1,&hellip;,S_t)$,那么下一个时刻状态为$S_{t&#43;1}$的概率为$P(S_{t&#43;1}\mid S_1,&hellip;,S_t)$
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质
$$ P(S_{t&#43;1}\mid S_t)=P(S_{t&#43;1}\mid S_1,&hellip;,S_t) $$ 也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。
注意：
虽然$t&#43;1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。
Markov process 通常用两元组$&lt;S,P&gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵
矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。
给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。
Markov reward process(MRP) 一个马尔科夫奖励过程由$\langle \mathcal{S},\mathcal{P},r,\gamma \rangle$
$\mathcal{S}$ 是有限状态的集合。 $\mathcal{P}$ 是状态转移矩阵。 $r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。 $\gamma$ 是折扣因子,$\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近0的$\gamma$ 更考虑短期奖励。 奖励函数的本质:向智能体传达目标(MDP) 强化学习的标准交互过程如下：每个时刻，智能体根据根据其 策略(policy)，在当前所处 状态(state) 选择一个 动作(action)，环境(environment) 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 奖励信号 (reward)，这通常是一个数值，奖励的折扣累加和称为 收益/回报 (return)，是智能体在动作选择过程中想要最大化的目标
“奖励 &amp; 收益” 其实是智能体目标的一种形式化、数值化的表征。可以把这种想法非正式地表述为 “收益假设” 收益是通过奖励信号计算的，而奖励函数是我们提供的，奖励函数起到了人与算法沟通的桥梁作用 需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标 回报 在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报（Return）
假设设置$\gamma =0.5$
$$ G_t=R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t&#43;k} $$ 假设路径为1,2,3,6
import numpy as np np.random.seed(0) #概率转移矩阵 P=[ [0.9,0.1,0.0,0.0,0.0,0.0], [0.5,0.0,0.5,0.0,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,0.4], [0.0,0.0,0.0,0.0,0.3,0.7], [0.0,0.2,0.3,0.5,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,1.0], ] P=np.array(P) rewards=[-1,-2,-2,10,1,0] gamma=0.5 def compute_chain_reward(start_index,chain,gamma): reward=0 for i in range(len(chain)): reward=gamma*reward&#43;rewards[chain[i]-1] return reward chain=[1,2,3,6] start_index=0 reward=compute_chain_reward(start_index,chain,gamma) print(reward) 状态价值函数 一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-&gt;价值
即其状态价值函数$V_{\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即
$V(s)=\mathbb{E}\left[G_t \mid S_t=s\right]$
展开为: $$ \begin{align} V(s) &amp;=\mathbb{E}[G_t \mid S_t=s] \ &amp;=\mathbb{E}[R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;&hellip;\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma(R_{t&#43;1}&#43;\gamma R_{t&#43;2})&#43;&hellip; \mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma G_{t&#43;1}\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma V(S_{t&#43;1})\mid S_t=s]\ \end{align} $$ 一方面,即时奖励的期望正是奖励函数的输出，即：
$\mathbb{E}\left[R_t \mid S_t=s\right]=r(s)$
另一方面，等式中剩余部分 $\mathbb{E}\left[\gamma V\left(S_{t&#43;1}\right) \mid S_t=s\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到 $$ V(s)=r(s)&#43;\gamma\sum_{s^{\prime} \in S }{p(s^{\prime} \mid s)V(s^\prime)} $$ 上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://niceasiv.cn/posts/reinforment_learning/" /><meta property="og:image" content="https://niceasiv.cn/images/papermod-cover.png"/><meta property="article:section" content="posts" />



<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://niceasiv.cn/images/papermod-cover.png"/>

<meta name="twitter:title" content="强化学习笔记"/>
<meta name="twitter:description" content="基础 马尔科夫决策(MDP) 在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。
如果已知历史的状态信息即$(S_1,&hellip;,S_t)$,那么下一个时刻状态为$S_{t&#43;1}$的概率为$P(S_{t&#43;1}\mid S_1,&hellip;,S_t)$
当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质
$$ P(S_{t&#43;1}\mid S_t)=P(S_{t&#43;1}\mid S_1,&hellip;,S_t) $$ 也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。
注意：
虽然$t&#43;1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。
Markov process 通常用两元组$&lt;S,P&gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵
矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。
给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。
Markov reward process(MRP) 一个马尔科夫奖励过程由$\langle \mathcal{S},\mathcal{P},r,\gamma \rangle$
$\mathcal{S}$ 是有限状态的集合。 $\mathcal{P}$ 是状态转移矩阵。 $r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。 $\gamma$ 是折扣因子,$\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近0的$\gamma$ 更考虑短期奖励。 奖励函数的本质:向智能体传达目标(MDP) 强化学习的标准交互过程如下：每个时刻，智能体根据根据其 策略(policy)，在当前所处 状态(state) 选择一个 动作(action)，环境(environment) 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 奖励信号 (reward)，这通常是一个数值，奖励的折扣累加和称为 收益/回报 (return)，是智能体在动作选择过程中想要最大化的目标
“奖励 &amp; 收益” 其实是智能体目标的一种形式化、数值化的表征。可以把这种想法非正式地表述为 “收益假设” 收益是通过奖励信号计算的，而奖励函数是我们提供的，奖励函数起到了人与算法沟通的桥梁作用 需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标 回报 在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报（Return）
假设设置$\gamma =0.5$
$$ G_t=R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t&#43;k} $$ 假设路径为1,2,3,6
import numpy as np np.random.seed(0) #概率转移矩阵 P=[ [0.9,0.1,0.0,0.0,0.0,0.0], [0.5,0.0,0.5,0.0,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,0.4], [0.0,0.0,0.0,0.0,0.3,0.7], [0.0,0.2,0.3,0.5,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,1.0], ] P=np.array(P) rewards=[-1,-2,-2,10,1,0] gamma=0.5 def compute_chain_reward(start_index,chain,gamma): reward=0 for i in range(len(chain)): reward=gamma*reward&#43;rewards[chain[i]-1] return reward chain=[1,2,3,6] start_index=0 reward=compute_chain_reward(start_index,chain,gamma) print(reward) 状态价值函数 一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-&gt;价值
即其状态价值函数$V_{\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即
$V(s)=\mathbb{E}\left[G_t \mid S_t=s\right]$
展开为: $$ \begin{align} V(s) &amp;=\mathbb{E}[G_t \mid S_t=s] \ &amp;=\mathbb{E}[R_t&#43;\gamma R_{t&#43;1}&#43;\gamma^2 R_{t&#43;2}&#43;&hellip;\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma(R_{t&#43;1}&#43;\gamma R_{t&#43;2})&#43;&hellip; \mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma G_{t&#43;1}\mid S_t=s]\ &amp;=\mathbb{E}[R_t&#43;\gamma V(S_{t&#43;1})\mid S_t=s]\ \end{align} $$ 一方面,即时奖励的期望正是奖励函数的输出，即：
$\mathbb{E}\left[R_t \mid S_t=s\right]=r(s)$
另一方面，等式中剩余部分 $\mathbb{E}\left[\gamma V\left(S_{t&#43;1}\right) \mid S_t=s\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到 $$ V(s)=r(s)&#43;\gamma\sum_{s^{\prime} \in S }{p(s^{\prime} \mid s)V(s^\prime)} $$ 上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [

        {
          "@type": "ListItem",
          "position":  2 ,
          "name": "Posts",
          "item": "https://niceasiv.cn/posts/"
        }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "强化学习笔记",
      "item": "https://niceasiv.cn/posts/reinforment_learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "强化学习笔记",
  "name": "强化学习笔记",
  "description": "基础 马尔科夫决策(MDP) 在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。\n如果已知历史的状态信息即$(S_1,\u0026hellip;,S_t)$,那么下一个时刻状态为$S_{t+1}$的概率为$P(S_{t+1}\\mid S_1,\u0026hellip;,S_t)$\n当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质\n$$ P(S_{t+1}\\mid S_t)=P(S_{t+1}\\mid S_1,\u0026hellip;,S_t) $$ 也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。\n注意：\n虽然$t+1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。\nMarkov process 通常用两元组$\u0026lt;S,P\u0026gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵\n矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。\n给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。\nMarkov reward process(MRP) 一个马尔科夫奖励过程由$\\langle \\mathcal{S},\\mathcal{P},r,\\gamma \\rangle$\n$\\mathcal{S}$ 是有限状态的集合。 $\\mathcal{P}$ 是状态转移矩阵。 $r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。 $\\gamma$ 是折扣因子,$\\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\\gamma$ 更关注长期的累计奖励，接近0的$\\gamma$ 更考虑短期奖励。 奖励函数的本质:向智能体传达目标(MDP) 强化学习的标准交互过程如下：每个时刻，智能体根据根据其 策略(policy)，在当前所处 状态(state) 选择一个 动作(action)，环境(environment) 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 奖励信号 (reward)，这通常是一个数值，奖励的折扣累加和称为 收益/回报 (return)，是智能体在动作选择过程中想要最大化的目标\n“奖励 \u0026amp; 收益” 其实是智能体目标的一种形式化、数值化的表征。可以把这种想法非正式地表述为 “收益假设” 收益是通过奖励信号计算的，而奖励函数是我们提供的，奖励函数起到了人与算法沟通的桥梁作用 需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标 回报 在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报（Return）\n假设设置$\\gamma =0.5$\n$$ G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots=\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} $$ 假设路径为1,2,3,6\nimport numpy as np np.random.seed(0) #概率转移矩阵 P=[ [0.9,0.1,0.0,0.0,0.0,0.0], [0.5,0.0,0.5,0.0,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,0.4], [0.0,0.0,0.0,0.0,0.3,0.7], [0.0,0.2,0.3,0.5,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,1.0], ] P=np.array(P) rewards=[-1,-2,-2,10,1,0] gamma=0.5 def compute_chain_reward(start_index,chain,gamma): reward=0 for i in range(len(chain)): reward=gamma*reward+rewards[chain[i]-1] return reward chain=[1,2,3,6] start_index=0 reward=compute_chain_reward(start_index,chain,gamma) print(reward) 状态价值函数 一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-\u0026gt;价值\n即其状态价值函数$V_{\\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即\n$V(s)=\\mathbb{E}\\left[G_t \\mid S_t=s\\right]$\n展开为: $$ \\begin{align} V(s) \u0026amp;=\\mathbb{E}[G_t \\mid S_t=s] \\ \u0026amp;=\\mathbb{E}[R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\u0026hellip;\\mid S_t=s]\\ \u0026amp;=\\mathbb{E}[R_t+\\gamma(R_{t+1}+\\gamma R_{t+2})+\u0026hellip; \\mid S_t=s]\\ \u0026amp;=\\mathbb{E}[R_t+\\gamma G_{t+1}\\mid S_t=s]\\ \u0026amp;=\\mathbb{E}[R_t+\\gamma V(S_{t+1})\\mid S_t=s]\\ \\end{align} $$ 一方面,即时奖励的期望正是奖励函数的输出，即：\n$\\mathbb{E}\\left[R_t \\mid S_t=s\\right]=r(s)$\n另一方面，等式中剩余部分 $\\mathbb{E}\\left[\\gamma V\\left(S_{t+1}\\right) \\mid S_t=s\\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到 $$ V(s)=r(s)+\\gamma\\sum_{s^{\\prime} \\in S }{p(s^{\\prime} \\mid s)V(s^\\prime)} $$ 上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。",
  "keywords": [
    "Machine Learning", "Reinforcement learning"
  ],
  "articleBody": "基础 马尔科夫决策(MDP) 在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。\n如果已知历史的状态信息即$(S_1,…,S_t)$,那么下一个时刻状态为$S_{t+1}$的概率为$P(S_{t+1}\\mid S_1,…,S_t)$\n当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有马尔可夫性质\n$$ P(S_{t+1}\\mid S_t)=P(S_{t+1}\\mid S_1,…,S_t) $$ 也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。\n注意：\n虽然$t+1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，历史的信息被传递到了现在。\nMarkov process 通常用两元组$$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵\n矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。\n给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态序列（episode），这个步骤也被叫做采样（sampling）。\nMarkov reward process(MRP) 一个马尔科夫奖励过程由$\\langle \\mathcal{S},\\mathcal{P},r,\\gamma \\rangle$\n$\\mathcal{S}$ 是有限状态的集合。 $\\mathcal{P}$ 是状态转移矩阵。 $r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。 $\\gamma$ 是折扣因子,$\\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\\gamma$ 更关注长期的累计奖励，接近0的$\\gamma$ 更考虑短期奖励。 奖励函数的本质:向智能体传达目标(MDP) 强化学习的标准交互过程如下：每个时刻，智能体根据根据其 策略(policy)，在当前所处 状态(state) 选择一个 动作(action)，环境(environment) 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 奖励信号 (reward)，这通常是一个数值，奖励的折扣累加和称为 收益/回报 (return)，是智能体在动作选择过程中想要最大化的目标\n“奖励 \u0026 收益” 其实是智能体目标的一种形式化、数值化的表征。可以把这种想法非正式地表述为 “收益假设” 收益是通过奖励信号计算的，而奖励函数是我们提供的，奖励函数起到了人与算法沟通的桥梁作用 需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标 回报 在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为回报（Return）\n假设设置$\\gamma =0.5$\n$$ G_t=R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+\\cdots=\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k} $$ 假设路径为1,2,3,6\nimport numpy as np np.random.seed(0) #概率转移矩阵 P=[ [0.9,0.1,0.0,0.0,0.0,0.0], [0.5,0.0,0.5,0.0,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,0.4], [0.0,0.0,0.0,0.0,0.3,0.7], [0.0,0.2,0.3,0.5,0.0,0.0], [0.0,0.0,0.0,0.6,0.0,1.0], ] P=np.array(P) rewards=[-1,-2,-2,10,1,0] gamma=0.5 def compute_chain_reward(start_index,chain,gamma): reward=0 for i in range(len(chain)): reward=gamma*reward+rewards[chain[i]-1] return reward chain=[1,2,3,6] start_index=0 reward=compute_chain_reward(start_index,chain,gamma) print(reward) 状态价值函数 一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-\u003e价值\n即其状态价值函数$V_{\\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即\n$V(s)=\\mathbb{E}\\left[G_t \\mid S_t=s\\right]$\n展开为: $$ \\begin{align} V(s) \u0026=\\mathbb{E}[G_t \\mid S_t=s] \\ \u0026=\\mathbb{E}[R_t+\\gamma R_{t+1}+\\gamma^2 R_{t+2}+…\\mid S_t=s]\\ \u0026=\\mathbb{E}[R_t+\\gamma(R_{t+1}+\\gamma R_{t+2})+… \\mid S_t=s]\\ \u0026=\\mathbb{E}[R_t+\\gamma G_{t+1}\\mid S_t=s]\\ \u0026=\\mathbb{E}[R_t+\\gamma V(S_{t+1})\\mid S_t=s]\\ \\end{align} $$ 一方面,即时奖励的期望正是奖励函数的输出，即：\n$\\mathbb{E}\\left[R_t \\mid S_t=s\\right]=r(s)$\n另一方面，等式中剩余部分 $\\mathbb{E}\\left[\\gamma V\\left(S_{t+1}\\right) \\mid S_t=s\\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到 $$ V(s)=r(s)+\\gamma\\sum_{s^{\\prime} \\in S }{p(s^{\\prime} \\mid s)V(s^\\prime)} $$ 上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。\n上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。\n若一个马尔可夫奖励过程一共有 $n$ 个状 态，即 $\\mathcal{S}=\\left{s_1, s_2, \\ldots, s_n\\right}$ ，我们将所有状态的价值表示成一个列向量 $\\mathcal{V}=\\left[V\\left(s_1\\right), V\\left(s_2\\right), \\ldots, V\\left(s_n\\right)\\right]^T$ ，同理，将奖励函数写成一个列向量 $\\mathcal{R}=\\left[r\\left(s_1\\right), r\\left(s_2\\right), \\ldots, r\\left(s_n\\right)\\right]^T$ 。于是我们可以将贝尔曼方程写成矩阵的形式: $$ \\begin{gathered} \\mathcal{V}=\\mathcal{R}+\\gamma \\mathcal{P} \\mathcal{V} \\ {\\left[\\begin{array}{c} V\\left(s_1\\right) \\ V\\left(s_2\\right) \\ \\ldots \\ V\\left(s_n\\right) \\end{array}\\right]=\\left[\\begin{array}{c} r\\left(s_1\\right) \\ r\\left(s_2\\right) \\ \\ldots \\ r\\left(s_n\\right) \\end{array}\\right]+\\gamma\\left[\\begin{array}{cccc} P\\left(s_1 \\mid s_1\\right) \u0026 p\\left(s_2 \\mid s_1\\right) \u0026 \\ldots \u0026 P\\left(s_n \\mid s_1\\right) \\ P\\left(s_1 \\mid s_2\\right) \u0026 P\\left(s_2 \\mid s_2\\right) \u0026 \\ldots \u0026 P\\left(s_n \\mid s_2\\right) \\ \\ldots \u0026 \u0026 \u0026 \\ P\\left(s_1 \\mid s_n\\right) \u0026 P\\left(s_2 \\mid s_n\\right) \u0026 \\ldots \u0026 P\\left(s_n \\mid s_n\\right) \\end{array}\\right]\\left[\\begin{array}{c} V\\left(s_1\\right) \\ V\\left(s_2\\right) \\ \\ldots \\ V\\left(s_n\\right) \\end{array}\\right]} \\end{gathered} $$ 我们可以直接根据矩阵运算求解，得到以下解析解: $$ \\begin{aligned} \\mathcal{V} \u0026 =\\mathcal{R}+\\gamma \\mathcal{P} \\mathcal{V} \\ (I-\\gamma \\mathcal{P}) \\mathcal{V} \u0026 =\\mathcal{R} \\ \\mathcal{V} \u0026 =(I-\\gamma \\mathcal{P})^{-1} \\mathcal{R} \\end{aligned} $$\ndef compute_state_value(P,rewards,gamma,states_num): rewards=np.array(rewards).reshape((-1,1)) value=np.dot(np.linalg.inv(np.eye(states_num)-gamma*P),rewards) return value state_value=compute_state_value(P,rewards,gamma,6) print(state_value) 动作价值函数 我们用 $Q^\\pi(s, a)$ 表示在 MDP 遵循策略 $\\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报: $$ Q^\\pi(s, a)=\\mathbb{E}\\pi\\left[G_t \\mid S_t=s, A_t=a\\right] $$ 状态价值函数和动作价值函数之间的关系：在使用策略 $\\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\\pi$ 采取所有动作的概率与相应的价值相乘 再求和的结果: $$ V^\\pi(s)=\\sum{a \\in A} \\pi(a \\mid s) Q^\\pi(s, a) $$ 使用策略 $\\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积: $$ Q^\\pi(s, a)=r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} P\\left(s^{\\prime} \\mid s, a\\right) V^\\pi\\left(s^{\\prime}\\right) $$\n贝尔曼期望方程 $$ \\begin{aligned} V^\\pi(s) \u0026 =\\mathbb{E}\\pi\\left[R_t+\\gamma V^\\pi\\left(S{t+1}\\right) \\mid S_t=s\\right] \\ \u0026 =\\sum_{a \\in A} \\pi(a \\mid s)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} p\\left(s^{\\prime} \\mid s, a\\right) V^\\pi\\left(s^{\\prime}\\right)\\right) \\ \\end{aligned} $$\n$$ \\begin{aligned} Q^\\pi(s, a) \u0026 =\\mathbb{E}\\pi\\left[R_t+\\gamma Q^\\pi\\left(S{t+1}, A_{t+1}\\right) \\mid S_t=s, A_t=a\\right] \\ \u0026 =r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} p\\left(s^{\\prime} \\mid s, a\\right) \\sum_{a^{\\prime} \\in A} \\pi\\left(a^{\\prime} \\mid s^{\\prime}\\right) Q^\\pi\\left(s^{\\prime}, a^{\\prime}\\right) \\end{aligned} $$\nMDP $\\langle \\mathcal{S},\\mathcal{A},P,r,\\gamma \\rangle$\n$\\mathcal{S}$是状态集合\n$\\mathcal{A}$ 是动作的集合；\n$\\gamma$ 是折扣因子;\n$r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$ ，在奖励函数 只取决于状态 $s$ 时，则退化为 $r(s)$ ；\n$P\\left(s^{\\prime} \\mid s, a\\right)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{\\prime}$ 的概 率。\n#mdp过程 state=['s1','s2','s3','s4','s5','s6'] action=['s1-\u003es1','s1-\u003es2','s2-\u003es1','s2-\u003es3','s3-\u003es4','s3-\u003es5','s4-\u003es5','s4-\u003es3','s4-\u003es4','s4-\u003es2'] #状态转移概率 P={ 's1-\u003es1':1.0, 's1-\u003es2':1.0, 's2-\u003es1':1.0, 's2-\u003es3':1.0, 's3-\u003es4':1.0, 's3-\u003es5':1.0, 's4-\u003es5':1.0, 's4-\u003es3':0.2, 's4-\u003es4':0.4, 's4-\u003es2':0.2, } #奖励函数 rewards={ 's1-\u003es1':-1.0, 's1-\u003es2':0, 's2-\u003es1':-1.0, 's2-\u003es3':-2.0, 's3-\u003es4':-2.0, 's3-\u003es5':0, 's4-\u003es5':10, 's4-\u003es3':1.0, 's4-\u003es4':1.0, 's4-\u003es2':1.0, } gamma=0.5 MDP=(state,action,P,rewards,gamma) #策略1 随机策略 Pi_1={ 's1-\u003es1':0.5, 's1-\u003es2':0.5, 's2-\u003es1':0.5, 's2-\u003es3':0.5, 's3-\u003es4':0.5, 's3-\u003es5':0.5, 's4-\u003es5':0.5, 's4-s*':0.5 } #策略2 Pi_2={ 's1-\u003es1':0.6, 's1-\u003es2':0.4, 's2-\u003es1':0.3, 's2-\u003es3':0.7, 's3-\u003es4':0.5, 's3-\u003es5':0.5, 's4-\u003es5':0.1, 's4-\u003es*':0.9 } 如何计算MDP下，一个策略$\\pi$的状态价值函数\nMDP-\u003eMRP\n奖励函数 $$ r^\\prime(s)=\\sum_{a \\in \\mathcal{A}}\\pi(a \\mid s)r(s,a) $$\n状态转移 $$ P^\\prime(s^\\prime|s)=\\sum_{a \\in \\mathcal{A}}{\\pi(a \\mid s)P(s^\\prime \\mid s,a)} $$\nMonte Carlo $$ V^\\pi(s)=\\mathbb{E}\\pi\\left[G_t \\mid S_t=s\\right] \\approx \\frac{1}{N} \\sum{i=1}^N G_t^{(i)} $$\n步骤:\n使用策略 $\\pi$ 采样若干条序列: $$ s_0^{(i)} \\stackrel{a_0^{(i)}}{\\longrightarrow} r_0^{(i)}, s_1^{(i)} \\stackrel{a_1^{(i)}}{\\longrightarrow} r_1^{(i)}, s_2^{(i)} \\stackrel{a_2^{(i)}}{\\longrightarrow} \\cdots \\stackrel{a_{T-1}^{(i)}}{\\longrightarrow} r_{T-1}^{(i)}, s_T^{(i)} $$ 对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作: 更新状态 $s$ 的计数器 $N(s) \\leftarrow N(s)+1$; 更新状态 $s$ 的总回报 $M(s) \\leftarrow M(s)+G_t$ ； 每一个状态的价值被估计为回报的平均值 $V(s)=M(s) / N(s)$ 。 根据大数定律，当 $N(s) \\rightarrow \\infty$ ，有 $V(s) \\rightarrow V^\\pi(s)$ 。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态 $s$ 和对应回报$G$ ，进行如下计算: $N(s) \\leftarrow N(s)+1$ $V(s) \\leftarrow V(s)+\\frac{1}{N(s)}(G-V(S))$ import numpy as np S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"] # 状态集合 A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"] # 动作集合 # 状态转移函数 P = { \"s1-保持s1-s1\": 1.0, \"s1-前往s2-s2\": 1.0, \"s2-前往s1-s1\": 1.0, \"s2-前往s3-s3\": 1.0, \"s3-前往s4-s4\": 1.0, \"s3-前往s5-s5\": 1.0, \"s4-前往s5-s5\": 1.0, \"s4-概率前往-s2\": 0.2, \"s4-概率前往-s3\": 0.4, \"s4-概率前往-s4\": 0.4, } # 奖励函数 R = { \"s1-保持s1\": -1, \"s1-前往s2\": 0, \"s2-前往s1\": -1, \"s2-前往s3\": -2, \"s3-前往s4\": -2, \"s3-前往s5\": 0, \"s4-前往s5\": 10, \"s4-概率前往\": 1, } gamma = 0.5 # 折扣因子 MDP = (S, A, P, R, gamma) # 策略1,随机策略 Pi_1 = { \"s1-保持s1\": 0.5, \"s1-前往s2\": 0.5, \"s2-前往s1\": 0.5, \"s2-前往s3\": 0.5, \"s3-前往s4\": 0.5, \"s3-前往s5\": 0.5, \"s4-前往s5\": 0.5, \"s4-概率前往\": 0.5, } # 策略2 Pi_2 = { \"s1-保持s1\": 0.6, \"s1-前往s2\": 0.4, \"s2-前往s1\": 0.3, \"s2-前往s3\": 0.7, \"s3-前往s4\": 0.5, \"s3-前往s5\": 0.5, \"s4-前往s5\": 0.1, \"s4-概率前往\": 0.9, } # 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量 def join(str1, str2): return str1 + '-' + str2 def sample(MDP, Pi, timestep_max, number): ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number ''' S, A, P, R, gamma = MDP episodes = [] for _ in range(number): episode = [] timestep = 0 s = S[np.random.randint(4)] # 随机选择一个除s5以外的状态s作为起点 # 当前状态为终止状态或者时间步太长时,一次采样结束 while s != \"s5\" and timestep \u003c= timestep_max: timestep += 1 rand, temp = np.random.rand(), 0 # 在状态s下根据策略选择动作 for a_opt in A: temp += Pi.get(join(s, a_opt), 0) if temp \u003e rand: a = a_opt r = R.get(join(s, a), 0) break rand, temp = np.random.rand(), 0 # 根据状态转移概率得到下一个状态s_next for s_opt in S: temp += P.get(join(join(s, a), s_opt), 0) if temp \u003e rand: s_next = s_opt break episode.append((s, a, r, s_next)) # 把（s,a,r,s_next）元组放入序列中 s = s_next # s_next变成当前状态,开始接下来的循环 episodes.append(episode) return episodes # # 采样5次,每个序列最长不超过20步 # episodes = sample(MDP, Pi_1, 20, 5)#策略为Pi_1 # 对所有采样序列计算所有状态的价值 def MC(episodes, V, N, gamma): for episode in episodes: G = 0 for i in range(len(episode) - 1, -1, -1): #一个序列从后往前计算 (s, a, r, s_next) = episode[i] G = r + gamma * G N[s] = N[s] + 1 V[s] = V[s] + (G - V[s]) / N[s] timestep_max = 20 # 采样1000次,可以自行修改 episodes = sample(MDP, Pi_1, timestep_max, 1000) gamma = 0.5 V = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0} N = {\"s1\": 0, \"s2\": 0, \"s3\": 0, \"s4\": 0, \"s5\": 0} MC(episodes, V, N, gamma) print(\"使用蒙特卡洛方法计算MDP的状态价值为\\n\", V) 最优策略 对于任意的状态 $s$ 都有 $V^\\pi(s) \\geq V^{\\pi^{\\prime}}(s)$ ，记 $\\pi\u003e\\pi^{\\prime}$ 。\n于是在有限状态 和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者 至少存在一个策略不差于其他所有策略，这个策略就是最优策略 (optimal policy) 。最优策略可能有很多个，我们都将其表示为 $\\pi^(s)$ 最优策略都有相同的状态价值函数，我们称之为最优状态价值函数， 表示为: $$ V^(s)=\\max \\pi V^\\pi(s), \\quad \\forall s \\in \\mathcal{S} $$ 同理，我们定义最优动作价值函数: $$ Q^(s, a)=\\max _\\pi Q^\\pi(s, a), \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A} $$ 最优状态价值函数和最优动作价值函数之间的关系： $$ Q^(s, a)=r(s,a)+\\gamma\\sum{s^\\prime \\in S}P(s^\\prime \\mid s,a)V^(s^\\prime) $$ 最优状态价值是选择此时使最优动作价值最大的那一个动作时的状态价值 $$ V^(s)=\\max_{a \\in \\mathcal{A}}Q^*(s,a) $$\nBellman最优方程 根据 $V^(s)$ 和 $Q^(s, a)$ 的关系，我们可以得到贝尔曼最优方程 (Bellman optimality equation) : $$ \\begin{gathered} V^(s)=\\max {a \\in \\mathcal{A}}\\left{r(s, a)+\\gamma \\sum{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) V^\\left(s^{\\prime}\\right)\\right} \\ Q^(s, a)=r(s, a)+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} p\\left(s^{\\prime} \\mid s, a\\right) \\max _{a^{\\prime} \\in \\mathcal{A}} Q^\\left(s^{\\prime}, a^{\\prime}\\right) \\end{gathered} $$\nDP优化 基于动态规划的强化学习算法主要有两种一是策略迭代,二是价值迭代\n策略迭代:策略评估(使用贝尔曼期望方程-\u003e策略的状态价值函数)+策略提升\n策略迭代算法 $$ \\begin{aligned} V^\\pi(s) \u0026 =\\mathbb{E}\\pi\\left[R_t+\\gamma V^\\pi\\left(S{t+1}\\right) \\mid S_t=s\\right] \\ \u0026 =\\sum_{a \\in A} \\pi(a \\mid s)\\left(r(s, a)+\\gamma \\sum_{s^{\\prime} \\in S} p\\left(s^{\\prime} \\mid s, a\\right) V^\\pi\\left(s^{\\prime}\\right)\\right) \\ \\end{aligned} $$\nDQN DDPG 参考文献 https://www.jianshu.com/p/1765772c8444\n",
  "wordCount" : "1078",
  "inLanguage": "zh",
  "datePublished": "0001-01-01T00:00:00Z",
  "dateModified": "0001-01-01T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Asiv"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://niceasiv.cn/posts/reinforment_learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Asiv's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://niceasiv.cn/img/logo.jpg"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    (function () {
        let  arr,reg = new RegExp("(^| )"+"change-themes"+"=([^;]*)(;|$)");
        if(arr = document.cookie.match(reg)) {
        } else {
            if (new Date().getHours() >= 19 || new Date().getHours() < 6) {
                document.body.classList.add('dark');
                localStorage.setItem("pref-theme", 'dark');
            } else {
                document.body.classList.remove('dark');
                localStorage.setItem("pref-theme", 'light');
            }
        }
    })()

    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }
</script>


<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://niceasiv.cn" accesskey="h" title="Asiv&#39;s Blog (Alt + H)">
            <img src="https://niceasiv.cn/img/logo.jpg" alt="logo" aria-label="logo"
                 height="35">Asiv&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                         fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                         stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://niceasiv.cn/search/" title="🔍 搜索 (Alt &#43; /)" accesskey=/>
                <span>🔍 搜索</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/" title="🏠 主页">
                <span>🏠 主页</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/posts" title="📚 文章">
                <span>📚 文章</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/tags" title="🧩 标签">
                <span>🧩 标签</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/archives/" title="⏱️ 时间轴">
                <span>⏱️ 时间轴</span>
                </a>
            </li>
            <li>
                <a href="https://niceasiv.cn/friends/" title="🤝 友链">
                <span>🤝 友链</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main page">
<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }
</style>

<article class="post-single">
    <div id="single-content">
        <header class="post-header">
            <div class="breadcrumbs"><a href="https://niceasiv.cn">🏠 主页</a>&nbsp;»&nbsp;<a href="https://niceasiv.cn/posts/">Posts</a></div>
            <h1 class="post-title">
                强化学习笔记
            </h1>
            <div class="post-meta">

<style>
    i[id*="post_meta_style"] {
        display: flex;
        align-items: center;
        margin: 0 0 10px 0;
    }

    .parent-post-meta {
        display: flex;
        flex-wrap: wrap;
        opacity: 0.8;
    }
</style>

<span class="parent-post-meta">
    <span id="post_meta_style_1">
        <span class="fa fa-calendar-check-o"></span>
        <span>0001-01-01
            &nbsp;&nbsp;
        </span>
    </span>
    
    
    
    
    
    
    
    <span id="post_meta_style_3">
        <span class="fa fa-file-word-o"></span>
        <span>1078字
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_4">
        <span class="fa fa-clock-o"></span>
        <span>6分钟
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_5">
        <span class="fa fa-user-o"></span>
        <span>Asiv
            &nbsp;&nbsp;
        </span>
    </span>
    <span id="post_meta_style_6">
        <span class="fa fa-tags" style="opacity: 0.8"></span>
        <span>
            <span class="post-tags-meta">
                <a href="https://niceasiv.cn/tags/machine-learning/" style="color: var(--secondary)!important;">Machine Learning</a>
                &nbsp;<a href="https://niceasiv.cn/tags/reinforcement-learning/" style="color: var(--secondary)!important;">Reinforcement learning</a>
            </span>
            &nbsp;&nbsp;
        </span>
    </span>
</span>
<span style="opacity: 0.8;">
                    <span id="post_meta_style_8">
                        <span class="fa fa-commenting-o"></span>
                        <span>
                            <script src="https://cdn.staticfile.org/twikoo/1.5.8/twikoo.all.min.js"></script>
                            <script>
                                let url = document.documentURI
                                
                                let dnsUrl = "https://niceasiv.cn"
                                let urlSplit = url.split(dnsUrl)
                                let finalUrl = urlSplit[1]
                                if (finalUrl[0] !== '/') {
                                    finalUrl = '/'+finalUrl
                                }
                                twikoo.getCommentsCount({
                                    envId: "https://twikoocdn.vercel.app/", 
                                region:  null , 
                                urls: [ 
                                    
                                    finalUrl,
                                ],
                                    includeReply: false 
                                }).then(function (res) {
                                    let count = res[0].count
                                    const obj = document.getElementById("comment_count");
                                    obj.innerText = count
                                    
                                    
                                    
                                }).catch(function (err) {
                                    
                                    console.error(err);
                                });
                            </script>
                            <span id="comment_count"></span>
                        </span>
                    </span>
                </span>

</div>
        </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e5%9f%ba%e7%a1%80" aria-label="基础">基础</a><ul>
                        
                <li>
                    <a href="#%e9%a9%ac%e5%b0%94%e7%a7%91%e5%a4%ab%e5%86%b3%e7%ad%96mdp" aria-label="马尔科夫决策(MDP)">马尔科夫决策(MDP)</a><ul>
                        
                <li>
                    <a href="#markov-process" aria-label="Markov process">Markov process</a></li>
                <li>
                    <a href="#markov-reward-processmrp" aria-label="Markov reward process(MRP)">Markov reward process(MRP)</a></li>
                <li>
                    <a href="#%e5%a5%96%e5%8a%b1%e5%87%bd%e6%95%b0%e7%9a%84%e6%9c%ac%e8%b4%a8%e5%90%91%e6%99%ba%e8%83%bd%e4%bd%93%e4%bc%a0%e8%be%be%e7%9b%ae%e6%a0%87mdp" aria-label="奖励函数的本质:向智能体传达目标(MDP)">奖励函数的本质:向智能体传达目标(MDP)</a></li>
                <li>
                    <a href="#%e5%9b%9e%e6%8a%a5" aria-label="回报">回报</a></li>
                <li>
                    <a href="#%e7%8a%b6%e6%80%81%e4%bb%b7%e5%80%bc%e5%87%bd%e6%95%b0" aria-label="状态价值函数">状态价值函数</a></li>
                <li>
                    <a href="#%e5%8a%a8%e4%bd%9c%e4%bb%b7%e5%80%bc%e5%87%bd%e6%95%b0" aria-label="动作价值函数">动作价值函数</a></li>
                <li>
                    <a href="#%e8%b4%9d%e5%b0%94%e6%9b%bc%e6%9c%9f%e6%9c%9b%e6%96%b9%e7%a8%8b" aria-label="贝尔曼期望方程">贝尔曼期望方程</a></li>
                <li>
                    <a href="#mdp" aria-label="MDP">MDP</a></li>
                <li>
                    <a href="#monte-carlo" aria-label="Monte Carlo">Monte Carlo</a></li>
                <li>
                    <a href="#%e6%9c%80%e4%bc%98%e7%ad%96%e7%95%a5" aria-label="最优策略">最优策略</a></li>
                <li>
                    <a href="#bellman%e6%9c%80%e4%bc%98%e6%96%b9%e7%a8%8b" aria-label="Bellman最优方程">Bellman最优方程</a></li></ul>
                </li>
                <li>
                    <a href="#dp%e4%bc%98%e5%8c%96" aria-label="DP优化">DP优化</a></li></ul>
                </li>
                <li>
                    <a href="#dqn" aria-label="DQN">DQN</a></li>
                <li>
                    <a href="#ddpg" aria-label="DDPG">DDPG</a></li>
                <li>
                    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae" aria-label="参考文献">参考文献</a>
                </li>
            </ul>
        </div>
        </details>
    </div>
</aside>
<script>
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
        
        activeElement = elements[0];
        const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
        document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
    }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        if (elements) {
            activeElement = Array.from(elements).find((element) => {
                if ((getOffsetTop(element) - window.pageYOffset) > 0 &&
                    (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                    return element;
                }
            }) || activeElement

            elements.forEach(element => {
                const id = encodeURI(element.getAttribute('id')).toLowerCase();
                if (element === activeElement){
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
                } else {
                    document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
                }
            })
        }
    }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;
        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;
    }
</script>
        <div class="post-content"><h2 id="基础">基础<a hidden class="anchor" aria-hidden="true" href="#基础">#</a></h2>
<h3 id="马尔科夫决策mdp">马尔科夫决策(MDP)<a hidden class="anchor" aria-hidden="true" href="#马尔科夫决策mdp">#</a></h3>
<p>在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。</p>
<p>如果已知历史的状态信息即$(S_1,&hellip;,S_t)$,那么下一个时刻状态为$S_{t+1}$的概率为$P(S_{t+1}\mid S_1,&hellip;,S_t)$</p>
<p>当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有<code>马尔可夫性质</code></p>
<p>$$
P(S_{t+1}\mid S_t)=P(S_{t+1}\mid S_1,&hellip;,S_t)
$$
也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。</p>
<p><code>注意</code>：</p>
<p>虽然$t+1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，<strong>历史的信息被传递到了现在。</strong></p>
<h4 id="markov-process">Markov process<a hidden class="anchor" aria-hidden="true" href="#markov-process">#</a></h4>
<p>通常用两元组$&lt;S,P&gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵</p>
<p>矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303131435145.png" alt="img"  />
</p>
<p>给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态<strong>序列</strong>（episode），这个步骤也被叫做<strong>采样</strong>（sampling）。</p>
<h4 id="markov-reward-processmrp">Markov reward process(MRP)<a hidden class="anchor" aria-hidden="true" href="#markov-reward-processmrp">#</a></h4>
<p>一个马尔科夫奖励过程由$\langle \mathcal{S},\mathcal{P},r,\gamma \rangle$</p>
<ul>
<li>$\mathcal{S}$ 是有限状态的集合。</li>
<li>$\mathcal{P}$ 是状态转移矩阵。</li>
<li>$r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。</li>
<li>$\gamma$ 是折扣因子,$\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近0的$\gamma$ 更考虑短期奖励。</li>
</ul>
<h4 id="奖励函数的本质向智能体传达目标mdp">奖励函数的本质:向智能体传达目标(MDP)<a hidden class="anchor" aria-hidden="true" href="#奖励函数的本质向智能体传达目标mdp">#</a></h4>
<p>强化学习的标准交互过程如下：每个时刻，智能体根据根据其 <code>策略(policy)</code>，在当前所处 <code>状态(state)</code> 选择一个 <code>动作(action)</code>，<code>环境(environment)</code> 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 <code>奖励信号 (reward)</code>，这通常是一个数值，奖励的折扣累加和称为 <code>收益/回报 (return)</code>，是智能体在动作选择过程中想要最大化的目标</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303141055151.webp" alt="强化学习交互图"  />
</p>
<ul>
<li><strong>“奖励 &amp; 收益” 其实是智能体目标的一种形式化、数值化的表征</strong>。可以把这种想法非正式地表述为 “收益假设”</li>
<li>收益是通过奖励信号计算的，而奖励函数是我们提供的，<strong>奖励函数起到了人与算法沟通的桥梁作用</strong></li>
<li>需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就<strong>必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标</strong></li>
</ul>
<h4 id="回报">回报<a hidden class="anchor" aria-hidden="true" href="#回报">#</a></h4>
<p>在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>（Return）</p>
<p>假设设置$\gamma =0.5$</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303131455336.png" alt="img"  />

$$
G_t=R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots=\sum_{k=0}^{\infty} \gamma^k R_{t+k}
$$
假设路径为1,2,3,6</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#概率转移矩阵</span>
</span></span><span class="line"><span class="cl"><span class="n">P</span><span class="o">=</span><span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.4</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.7</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.3</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">0.6</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">1.0</span><span class="p">],</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">P</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">P</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">rewards</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_chain_reward</span><span class="p">(</span><span class="n">start_index</span><span class="p">,</span><span class="n">chain</span><span class="p">,</span><span class="n">gamma</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">reward</span><span class="o">=</span><span class="mi">0</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">chain</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">reward</span><span class="o">=</span><span class="n">gamma</span><span class="o">*</span><span class="n">reward</span><span class="o">+</span><span class="n">rewards</span><span class="p">[</span><span class="n">chain</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">reward</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">chain</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">start_index</span><span class="o">=</span><span class="mi">0</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">reward</span><span class="o">=</span><span class="n">compute_chain_reward</span><span class="p">(</span><span class="n">start_index</span><span class="p">,</span><span class="n">chain</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="状态价值函数">状态价值函数<a hidden class="anchor" aria-hidden="true" href="#状态价值函数">#</a></h4>
<p>一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-&gt;价值</p>
<p>即其状态价值函数$V_{\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即</p>
<p>$V(s)=\mathbb{E}\left[G_t \mid S_t=s\right]$</p>
<p>展开为:
$$
\begin{align}
V(s) &amp;=\mathbb{E}[G_t \mid S_t=s] \
&amp;=\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+&hellip;\mid S_t=s]\
&amp;=\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2})+&hellip; \mid S_t=s]\
&amp;=\mathbb{E}[R_t+\gamma G_{t+1}\mid S_t=s]\
&amp;=\mathbb{E}[R_t+\gamma V(S_{t+1})\mid S_t=s]\
\end{align}
$$
一方面,即时奖励的期望正是奖励函数的输出，即：</p>
<p>$\mathbb{E}\left[R_t \mid S_t=s\right]=r(s)$</p>
<p>另一方面，等式中剩余部分 $\mathbb{E}\left[\gamma V\left(S_{t+1}\right) \mid S_t=s\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到
$$
V(s)=r(s)+\gamma\sum_{s^{\prime} \in S }{p(s^{\prime} \mid s)V(s^\prime)}
$$
上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303141138303.jpeg" alt="img"  />
</p>
<p>上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。</p>
<p>若一个马尔可夫奖励过程一共有 $n$ 个状 态，即 $\mathcal{S}=\left{s_1, s_2, \ldots, s_n\right}$ ，我们将所有状态的价值表示成一个列向量 $\mathcal{V}=\left[V\left(s_1\right), V\left(s_2\right), \ldots, V\left(s_n\right)\right]^T$ ，同理，将奖励函数写成一个列向量 $\mathcal{R}=\left[r\left(s_1\right), r\left(s_2\right), \ldots, r\left(s_n\right)\right]^T$ 。于是我们可以将贝尔曼方程写成矩阵的形式:
$$
\begin{gathered}
\mathcal{V}=\mathcal{R}+\gamma \mathcal{P} \mathcal{V} \
{\left[\begin{array}{c}
V\left(s_1\right) \
V\left(s_2\right) \
\ldots \
V\left(s_n\right)
\end{array}\right]=\left[\begin{array}{c}
r\left(s_1\right) \
r\left(s_2\right) \
\ldots \
r\left(s_n\right)
\end{array}\right]+\gamma\left[\begin{array}{cccc}
P\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; P\left(s_n \mid s_1\right) \
P\left(s_1 \mid s_2\right) &amp; P\left(s_2 \mid s_2\right) &amp; \ldots &amp; P\left(s_n \mid s_2\right) \
\ldots &amp; &amp; &amp; \
P\left(s_1 \mid s_n\right) &amp; P\left(s_2 \mid s_n\right) &amp; \ldots &amp; P\left(s_n \mid s_n\right)
\end{array}\right]\left[\begin{array}{c}
V\left(s_1\right) \
V\left(s_2\right) \
\ldots \
V\left(s_n\right)
\end{array}\right]}
\end{gathered}
$$
我们可以直接根据矩阵运算求解，得到以下解析解:
$$
\begin{aligned}
\mathcal{V} &amp; =\mathcal{R}+\gamma \mathcal{P} \mathcal{V} \
(I-\gamma \mathcal{P}) \mathcal{V} &amp; =\mathcal{R} \
\mathcal{V} &amp; =(I-\gamma \mathcal{P})^{-1} \mathcal{R}
\end{aligned}
$$</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">compute_state_value</span><span class="p">(</span><span class="n">P</span><span class="p">,</span><span class="n">rewards</span><span class="p">,</span><span class="n">gamma</span><span class="p">,</span><span class="n">states_num</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">rewards</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</span></span><span class="line"><span class="cl">    <span class="n">value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">states_num</span><span class="p">)</span><span class="o">-</span><span class="n">gamma</span><span class="o">*</span><span class="n">P</span><span class="p">),</span><span class="n">rewards</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">value</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">state_value</span><span class="o">=</span><span class="n">compute_state_value</span><span class="p">(</span><span class="n">P</span><span class="p">,</span><span class="n">rewards</span><span class="p">,</span><span class="n">gamma</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">state_value</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="动作价值函数">动作价值函数<a hidden class="anchor" aria-hidden="true" href="#动作价值函数">#</a></h4>
<p>我们用 $Q^\pi(s, a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报:
$$
Q^\pi(s, a)=\mathbb{E}<em>\pi\left[G_t \mid S_t=s, A_t=a\right]
$$
状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘 再求和的结果:
$$
V^\pi(s)=\sum</em>{a \in A} \pi(a \mid s) Q^\pi(s, a)
$$
使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积:
$$
Q^\pi(s, a)=r(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)
$$</p>
<h4 id="贝尔曼期望方程">贝尔曼期望方程<a hidden class="anchor" aria-hidden="true" href="#贝尔曼期望方程">#</a></h4>
<p>$$
\begin{aligned}
V^\pi(s) &amp; =\mathbb{E}<em>\pi\left[R_t+\gamma V^\pi\left(S</em>{t+1}\right) \mid S_t=s\right] \
&amp; =\sum_{a \in A} \pi(a \mid s)\left(r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right) \
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
Q^\pi(s, a) &amp; =\mathbb{E}<em>\pi\left[R_t+\gamma Q^\pi\left(S</em>{t+1}, A_{t+1}\right) \mid S_t=s, A_t=a\right] \
&amp; =r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q^\pi\left(s^{\prime}, a^{\prime}\right)
\end{aligned}
$$</p>
<h4 id="mdp">MDP<a hidden class="anchor" aria-hidden="true" href="#mdp">#</a></h4>
<p>$\langle \mathcal{S},\mathcal{A},P,r,\gamma \rangle$</p>
<ul>
<li>
<p>$\mathcal{S}$是状态集合</p>
</li>
<li>
<p>$\mathcal{A}$ 是动作的集合；</p>
</li>
<li>
<p>$\gamma$ 是折扣因子;</p>
</li>
<li>
<p>$r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$ ，在奖励函数 只取决于状态 $s$ 时，则退化为 $r(s)$ ；</p>
</li>
<li>
<p>$P\left(s^{\prime} \mid s, a\right)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{\prime}$ 的概 率。</p>
</li>
</ul>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303141055151.webp" alt="强化学习交互图"  />
</p>
<p><img loading="lazy" src="https://cdn.niceasiv.cn/202303151058328.png" alt="img"  />
</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1">#mdp过程</span>
</span></span><span class="line"><span class="cl"><span class="n">state</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;s1&#39;</span><span class="p">,</span><span class="s1">&#39;s2&#39;</span><span class="p">,</span><span class="s1">&#39;s3&#39;</span><span class="p">,</span><span class="s1">&#39;s4&#39;</span><span class="p">,</span><span class="s1">&#39;s5&#39;</span><span class="p">,</span><span class="s1">&#39;s6&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">action</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;s1-&gt;s1&#39;</span><span class="p">,</span><span class="s1">&#39;s1-&gt;s2&#39;</span><span class="p">,</span><span class="s1">&#39;s2-&gt;s1&#39;</span><span class="p">,</span><span class="s1">&#39;s2-&gt;s3&#39;</span><span class="p">,</span><span class="s1">&#39;s3-&gt;s4&#39;</span><span class="p">,</span><span class="s1">&#39;s3-&gt;s5&#39;</span><span class="p">,</span><span class="s1">&#39;s4-&gt;s5&#39;</span><span class="p">,</span><span class="s1">&#39;s4-&gt;s3&#39;</span><span class="p">,</span><span class="s1">&#39;s4-&gt;s4&#39;</span><span class="p">,</span><span class="s1">&#39;s4-&gt;s2&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="c1">#状态转移概率</span>
</span></span><span class="line"><span class="cl"><span class="n">P</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s1&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s2&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s1&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s3&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s4&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s5&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s5&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s3&#39;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s4&#39;</span><span class="p">:</span><span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s2&#39;</span><span class="p">:</span><span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#奖励函数</span>
</span></span><span class="line"><span class="cl"><span class="n">rewards</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s1&#39;</span><span class="p">:</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s2&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s1&#39;</span><span class="p">:</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s3&#39;</span><span class="p">:</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s4&#39;</span><span class="p">:</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s5&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s5&#39;</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s3&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s4&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s2&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">MDP</span><span class="o">=</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">P</span><span class="p">,</span><span class="n">rewards</span><span class="p">,</span><span class="n">gamma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#策略1 随机策略</span>
</span></span><span class="line"><span class="cl"><span class="n">Pi_1</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s1&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s2&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s1&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s3&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s4&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s5&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s5&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-s*&#39;</span><span class="p">:</span><span class="mf">0.5</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#策略2</span>
</span></span><span class="line"><span class="cl"><span class="n">Pi_2</span><span class="o">=</span><span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s1&#39;</span><span class="p">:</span><span class="mf">0.6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s1-&gt;s2&#39;</span><span class="p">:</span><span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s1&#39;</span><span class="p">:</span><span class="mf">0.3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s2-&gt;s3&#39;</span><span class="p">:</span><span class="mf">0.7</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s4&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s3-&gt;s5&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s5&#39;</span><span class="p">:</span><span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;s4-&gt;s*&#39;</span><span class="p">:</span><span class="mf">0.9</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span></code></pre></div><p>如何计算MDP下，一个策略$\pi$的状态价值函数</p>
<p>MDP-&gt;MRP</p>
<p>奖励函数
$$
r^\prime(s)=\sum_{a \in \mathcal{A}}\pi(a \mid s)r(s,a)
$$</p>
<p>状态转移
$$
P^\prime(s^\prime|s)=\sum_{a \in \mathcal{A}}{\pi(a \mid s)P(s^\prime \mid s,a)}
$$</p>
<h4 id="monte-carlo">Monte Carlo<a hidden class="anchor" aria-hidden="true" href="#monte-carlo">#</a></h4>
<p>$$
V^\pi(s)=\mathbb{E}<em>\pi\left[G_t \mid S_t=s\right] \approx \frac{1}{N} \sum</em>{i=1}^N G_t^{(i)}
$$</p>
<p><code>步骤</code>:</p>
<ol>
<li>使用策略 $\pi$ 采样若干条序列:
$$
s_0^{(i)} \stackrel{a_0^{(i)}}{\longrightarrow} r_0^{(i)}, s_1^{(i)} \stackrel{a_1^{(i)}}{\longrightarrow} r_1^{(i)}, s_2^{(i)} \stackrel{a_2^{(i)}}{\longrightarrow} \cdots \stackrel{a_{T-1}^{(i)}}{\longrightarrow} r_{T-1}^{(i)}, s_T^{(i)}
$$</li>
<li>对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作:</li>
</ol>
<ul>
<li>更新状态 $s$ 的计数器 $N(s) \leftarrow N(s)+1$;</li>
<li>更新状态 $s$ 的总回报 $M(s) \leftarrow M(s)+G_t$ ；</li>
</ul>
<ol start="3">
<li>每一个状态的价值被估计为回报的平均值 $V(s)=M(s) / N(s)$ 。
根据大数定律，当 $N(s) \rightarrow \infty$ ，有 $V(s) \rightarrow V^\pi(s)$ 。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态 $s$ 和对应回报$G$ ，进行如下计算:</li>
</ol>
<ul>
<li>$N(s) \leftarrow N(s)+1$</li>
<li>$V(s) \leftarrow V(s)+\frac{1}{N(s)}(G-V(S))$</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="n">S</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;s1&#34;</span><span class="p">,</span> <span class="s2">&#34;s2&#34;</span><span class="p">,</span> <span class="s2">&#34;s3&#34;</span><span class="p">,</span> <span class="s2">&#34;s4&#34;</span><span class="p">,</span> <span class="s2">&#34;s5&#34;</span><span class="p">]</span>  <span class="c1"># 状态集合</span>
</span></span><span class="line"><span class="cl"><span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;保持s1&#34;</span><span class="p">,</span> <span class="s2">&#34;前往s1&#34;</span><span class="p">,</span> <span class="s2">&#34;前往s2&#34;</span><span class="p">,</span> <span class="s2">&#34;前往s3&#34;</span><span class="p">,</span> <span class="s2">&#34;前往s4&#34;</span><span class="p">,</span> <span class="s2">&#34;前往s5&#34;</span><span class="p">,</span> <span class="s2">&#34;概率前往&#34;</span><span class="p">]</span>  <span class="c1"># 动作集合</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 状态转移函数</span>
</span></span><span class="line"><span class="cl"><span class="n">P</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-保持s1-s1&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-前往s2-s2&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s1-s1&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s3-s3&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s4-s4&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s5-s5&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-前往s5-s5&#34;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往-s2&#34;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往-s3&#34;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往-s4&#34;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 奖励函数</span>
</span></span><span class="line"><span class="cl"><span class="n">R</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-保持s1&#34;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-前往s2&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s1&#34;</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s3&#34;</span><span class="p">:</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s4&#34;</span><span class="p">:</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s5&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-前往s5&#34;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往&#34;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># 折扣因子</span>
</span></span><span class="line"><span class="cl"><span class="n">MDP</span> <span class="o">=</span> <span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 策略1,随机策略</span>
</span></span><span class="line"><span class="cl"><span class="n">Pi_1</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-保持s1&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-前往s2&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s1&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s3&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s4&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s5&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-前往s5&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 策略2</span>
</span></span><span class="line"><span class="cl"><span class="n">Pi_2</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-保持s1&#34;</span><span class="p">:</span> <span class="mf">0.6</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s1-前往s2&#34;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s1&#34;</span><span class="p">:</span> <span class="mf">0.3</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s2-前往s3&#34;</span><span class="p">:</span> <span class="mf">0.7</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s4&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s3-前往s5&#34;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-前往s5&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;s4-概率前往&#34;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">join</span><span class="p">(</span><span class="n">str1</span><span class="p">,</span> <span class="n">str2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">str1</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span> <span class="o">+</span> <span class="n">str2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">MDP</span><span class="p">,</span> <span class="n">Pi</span><span class="p">,</span> <span class="n">timestep_max</span><span class="p">,</span> <span class="n">number</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;&#39;&#39; 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number &#39;&#39;&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">S</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">gamma</span> <span class="o">=</span> <span class="n">MDP</span>
</span></span><span class="line"><span class="cl">    <span class="n">episodes</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">episode</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">        <span class="n">timestep</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="n">s</span> <span class="o">=</span> <span class="n">S</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span>  <span class="c1"># 随机选择一个除s5以外的状态s作为起点</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># 当前状态为终止状态或者时间步太长时,一次采样结束</span>
</span></span><span class="line"><span class="cl">        <span class="k">while</span> <span class="n">s</span> <span class="o">!=</span> <span class="s2">&#34;s5&#34;</span> <span class="ow">and</span> <span class="n">timestep</span> <span class="o">&lt;=</span> <span class="n">timestep_max</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">timestep</span> <span class="o">+=</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="n">rand</span><span class="p">,</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(),</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 在状态s下根据策略选择动作</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">a_opt</span> <span class="ow">in</span> <span class="n">A</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">temp</span> <span class="o">+=</span> <span class="n">Pi</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a_opt</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">temp</span> <span class="o">&gt;</span> <span class="n">rand</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">a</span> <span class="o">=</span> <span class="n">a_opt</span>
</span></span><span class="line"><span class="cl">                    <span class="n">r</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                    <span class="k">break</span>
</span></span><span class="line"><span class="cl">            <span class="n">rand</span><span class="p">,</span> <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(),</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 根据状态转移概率得到下一个状态s_next</span>
</span></span><span class="line"><span class="cl">            <span class="k">for</span> <span class="n">s_opt</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">temp</span> <span class="o">+=</span> <span class="n">P</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">join</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">s_opt</span><span class="p">),</span> <span class="mi">0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">                <span class="k">if</span> <span class="n">temp</span> <span class="o">&gt;</span> <span class="n">rand</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                    <span class="n">s_next</span> <span class="o">=</span> <span class="n">s_opt</span>
</span></span><span class="line"><span class="cl">                    <span class="k">break</span>
</span></span><span class="line"><span class="cl">            <span class="n">episode</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_next</span><span class="p">))</span>  <span class="c1"># 把（s,a,r,s_next）元组放入序列中</span>
</span></span><span class="line"><span class="cl">            <span class="n">s</span> <span class="o">=</span> <span class="n">s_next</span>  <span class="c1"># s_next变成当前状态,开始接下来的循环</span>
</span></span><span class="line"><span class="cl">        <span class="n">episodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">episodes</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># # 采样5次,每个序列最长不超过20步</span>
</span></span><span class="line"><span class="cl"><span class="c1"># episodes = sample(MDP, Pi_1, 20, 5)#策略为Pi_1</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对所有采样序列计算所有状态的价值</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">MC</span><span class="p">(</span><span class="n">episodes</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="n">episodes</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">episode</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>  <span class="c1">#一个序列从后往前计算</span>
</span></span><span class="line"><span class="cl">            <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_next</span><span class="p">)</span> <span class="o">=</span> <span class="n">episode</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="n">G</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span>
</span></span><span class="line"><span class="cl">            <span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">            <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">G</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">s</span><span class="p">])</span> <span class="o">/</span> <span class="n">N</span><span class="p">[</span><span class="n">s</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">timestep_max</span> <span class="o">=</span> <span class="mi">20</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 采样1000次,可以自行修改</span>
</span></span><span class="line"><span class="cl"><span class="n">episodes</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="n">MDP</span><span class="p">,</span> <span class="n">Pi_1</span><span class="p">,</span> <span class="n">timestep_max</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.5</span>
</span></span><span class="line"><span class="cl"><span class="n">V</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;s1&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s2&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s3&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s4&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s5&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">N</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&#34;s1&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s2&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s3&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s4&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&#34;s5&#34;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
</span></span><span class="line"><span class="cl"><span class="n">MC</span><span class="p">(</span><span class="n">episodes</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;使用蒙特卡洛方法计算MDP的状态价值为</span><span class="se">\n</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</span></span></code></pre></div><h4 id="最优策略">最优策略<a hidden class="anchor" aria-hidden="true" href="#最优策略">#</a></h4>
<p>对于任意的状态 $s$ 都有 $V^\pi(s) \geq V^{\pi^{\prime}}(s)$ ，记 $\pi&gt;\pi^{\prime}$ 。</p>
<p>于是在有限状态 和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者 至少存在一个策略不差于其他所有策略，这个策略就是<code>最优策略 (optimal policy)</code> 。最优策略可能有很多个，我们都将其表示为 $\pi^<em>(s)$
最优策略都有相同的状态价值函数，我们称之为最优状态价值函数， 表示为:
$$
V^</em>(s)=\max <em>\pi V^\pi(s), \quad \forall s \in \mathcal{S}
$$
同理，我们定义最优动作价值函数:
$$
Q^<em>(s, a)=\max _\pi Q^\pi(s, a), \quad \forall s \in \mathcal{S}, a \in \mathcal{A}
$$
最优状态价值函数和最优动作价值函数之间的关系：
$$
Q^</em>(s, a)=r(s,a)+\gamma\sum</em>{s^\prime \in S}P(s^\prime \mid s,a)V^<em>(s^\prime)
$$
最优状态价值是选择此时使<code>最优动作价值</code>最大的那一个动作时的状态价值
$$
V^</em>(s)=\max_{a \in \mathcal{A}}Q^*(s,a)
$$</p>
<h4 id="bellman最优方程">Bellman最优方程<a hidden class="anchor" aria-hidden="true" href="#bellman最优方程">#</a></h4>
<p>根据 $V^<em>(s)$ 和 $Q^</em>(s, a)$ 的关系，我们可以得到贝尔曼最优方程
(Bellman optimality equation) :
$$
\begin{gathered}
V^<em>(s)=\max <em>{a \in \mathcal{A}}\left{r(s, a)+\gamma \sum</em>{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) V^</em>\left(s^{\prime}\right)\right} \
Q^<em>(s, a)=r(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) \max _{a^{\prime} \in \mathcal{A}} Q^</em>\left(s^{\prime}, a^{\prime}\right)
\end{gathered}
$$</p>
<h3 id="dp优化">DP优化<a hidden class="anchor" aria-hidden="true" href="#dp优化">#</a></h3>
<p>基于动态规划的强化学习算法主要有两种一是<code>策略迭代</code>,二是<code>价值迭代</code></p>
<p>策略迭代:策略评估(使用贝尔曼期望方程-&gt;策略的状态价值函数)+策略提升</p>
<p><code>策略迭代算法</code>
$$
\begin{aligned}
V^\pi(s) &amp; =\mathbb{E}<em>\pi\left[R_t+\gamma V^\pi\left(S</em>{t+1}\right) \mid S_t=s\right] \
&amp; =\sum_{a \in A} \pi(a \mid s)\left(r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right) \
\end{aligned}
$$</p>
<h2 id="dqn">DQN<a hidden class="anchor" aria-hidden="true" href="#dqn">#</a></h2>
<h2 id="ddpg">DDPG<a hidden class="anchor" aria-hidden="true" href="#ddpg">#</a></h2>
<h2 id="参考文献">参考文献<a hidden class="anchor" aria-hidden="true" href="#参考文献">#</a></h2>
<p><a href="https://www.jianshu.com/p/1765772c8444">https://www.jianshu.com/p/1765772c8444</a></p>


        </div>

        <footer class="post-footer">
            
<nav class="paginav">
  <a class="prev" href="https://niceasiv.cn/posts/policy_distillation/">
    <span class="title">« 上一页</span>
    <br>
    <span>策略蒸馏</span>
  </a>
  <a class="next" href="https://niceasiv.cn/posts/algorithm_basis/">
    <span class="title">下一页 »</span>
    <br>
    <span>算法与程序设计基础&amp;&amp;CSP入门学习</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on twitter"
       href="https://twitter.com/intent/tweet/?text=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f&amp;hashtags=MachineLearning%2cReinforcementlearning">
    <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on linkedin"
       href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f&amp;title=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0&amp;summary=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0&amp;source=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on reddit"
       href="https://reddit.com/submit?url=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f&title=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on facebook"
       href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on whatsapp"
       href="https://api.whatsapp.com/send?text=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0%20-%20https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share 强化学习笔记 on telegram"
       href="https://telegram.me/share/url?text=%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%ac%94%e8%ae%b0&amp;url=https%3a%2f%2fniceasiv.cn%2fposts%2freinforment_learning%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

        </footer>
    </div>
</article>
</main>

<footer class="footer">
    <span>
        Copyright
        &copy;
        2021-2023 Asiv&#39;s Blog
    </span>
    <a href="https://beian.miit.gov.cn/" target="_blank" style="color:#939393;">陕ICP备2021000000号</a>&nbsp;
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <span class="topInner">
        <svg class="topSvg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z"/>
        </svg>
        <span id="read_progress"></span>
    </span>
</a>

<script>
    document.addEventListener('scroll', function (e) {
        const readProgress = document.getElementById("read_progress");
        const scrollHeight = document.documentElement.scrollHeight;
        const clientHeight = document.documentElement.clientHeight;
        const scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
        readProgress.innerText = ((scrollTop / (scrollHeight - clientHeight)).toFixed(2) * 100).toFixed(0);
    })
</script>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });
</script>
<script>
    let mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 200 || document.documentElement.scrollTop > 200) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        (function() {
            document.cookie = "change-themes" + "="+ escape ("false");
        })()

        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    });
</script>

<script>
    document.body.addEventListener('copy', function (e) {
        if (window.getSelection().toString() && window.getSelection().toString().length > 50) {
            let clipboardData = e.clipboardData || window.clipboardData;
            if (clipboardData) {
                e.preventDefault();
                let htmlData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Asiv's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                let textData = window.getSelection().toString() +
                    '\r\n\n————————————————\r\n' +
                    '版权声明：本文为「'+"Asiv's Blog"+'」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。' +
                '\r\n原文链接：' + location.href;
                clipboardData.setData('text/html', htmlData);
                clipboardData.setData('text/plain', textData);
            }
        }
    });
</script></body>

</html>
