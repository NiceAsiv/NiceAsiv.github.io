<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>强化学习笔记 - Asiv&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/img/avatar.jpg">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="强化学习笔记 - Asiv&#39;s Blog" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://niceasiv.cn/2023/03/04/reinforment_learning/index.html" />
  
  <meta property="og:image" content="/favicon.png" />
  
  <meta property="og:article:published_time" content="2023-03-04T09:06:09.000Z" />
  
  <meta property="og:article:author" content="Asiv" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-rainbow-banner="true"
        data-rainbow-banner-shown="auto"
        data-rainbow-banner-month="3"
        data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="3"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Asiv&#39;s Blog</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/friends">Friends</a>
            
            
            
            <a class="nav-item" href="/essay">Essay</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/NiceAsiv" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        
        <div class="categories text-uppercase">
        
            <a href="/categories/Machine-Learning/">Machine Learning</a>
        
        </div>
        

        
        <div class="date" id="date">
            <span>March</span>
            <span>4,</span>
            <span>2023</span>
        </div>
        

        <h2 class="title">强化学习笔记</h2>
    </div>

    <div class="divider"></div>

    <div class="content">
        <h2 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h2><h3 id="马尔科夫决策-MDP"><a href="#马尔科夫决策-MDP" class="headerlink" title="马尔科夫决策(MDP)"></a>马尔科夫决策(MDP)</h3><p>在随机过程中某时刻$t$的状态用$S_t$表示，所以可能的状态组成了状态空间$S$。</p>
<p>如果已知历史的状态信息即$(S_1,…,S_t)$,那么下一个时刻状态为$S_{t+1}$的概率为$P(S_{t+1}\mid S_1,…,S_t)$</p>
<p>当且仅当某时刻的状态只取决于上一时刻的状态时，一个随机过程被称为具有<code>马尔可夫性质</code></p>
<p>$$<br>P(S_{t+1}\mid S_t)&#x3D;P(S_{t+1}\mid S_1,…,S_t)<br>$$<br>也就是说当前状态是未来的充分统计量，即下一个状态只取决于当前状态，而不会受到过去状态的影响。</p>
<p><code>注意</code>：</p>
<p>虽然$t+1$时刻的状态只与$t$时刻的状态有关，但是$t$时刻的状态其实包含了$t-1$时刻的状态的信息，通过这种链式的关系，<strong>历史的信息被传递到了现在。</strong></p>
<h4 id="Markov-process"><a href="#Markov-process" class="headerlink" title="Markov process"></a>Markov process</h4><p>通常用两元组$&lt;S,P&gt;$描述一个马尔科夫过程，其中$S$是有限状态集合，$P$是状态转移矩阵</p>
<p>矩阵$P$中第$i$行第$j$列$P(s_j|s_i)$表示状态$s_i$转移到状态$s_j$的概率，从某个状态出发，到达其他状态的概率和必须为1，即状态转移矩阵的每一行的和为1。</p>
<p><img src="https://cdn.niceasiv.cn/202303131435145.png" alt="img"></p>
<p>给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态<strong>序列</strong>（episode），这个步骤也被叫做<strong>采样</strong>（sampling）。</p>
<h4 id="Markov-reward-process-MRP"><a href="#Markov-reward-process-MRP" class="headerlink" title="Markov reward process(MRP)"></a>Markov reward process(MRP)</h4><p>一个马尔科夫奖励过程由$\langle \mathcal{S},\mathcal{P},r,\gamma \rangle$</p>
<ul>
<li>$\mathcal{S}$ 是有限状态的集合。</li>
<li>$\mathcal{P}$ 是状态转移矩阵。</li>
<li>$r$是奖励函数,某个状态$s$的奖励$r(s)$指转移到该状态时可以获得奖励的期望。</li>
<li>$\gamma$ 是折扣因子,$\gamma$的取值为$[0,1)$ 。引入折扣因子的理由为远期利益具有一定不确定性，有时我们更希望能够尽快获得一些奖励，所以我们需要对远期利益打一些折扣。接近 1 的 $\gamma$ 更关注长期的累计奖励，接近0的$\gamma$ 更考虑短期奖励。</li>
</ul>
<h4 id="奖励函数的本质-向智能体传达目标-MDP"><a href="#奖励函数的本质-向智能体传达目标-MDP" class="headerlink" title="奖励函数的本质:向智能体传达目标(MDP)"></a>奖励函数的本质:向智能体传达目标(MDP)</h4><p>强化学习的标准交互过程如下：每个时刻，智能体根据根据其 <code>策略(policy)</code>，在当前所处 <code>状态(state)</code> 选择一个 <code>动作(action)</code>，<code>环境(environment)</code> 对这些动作做出相应的相应的响应，转移到新状态，同时产生一个 <code>奖励信号 (reward)</code>，这通常是一个数值，奖励的折扣累加和称为 <code>收益/回报 (return)</code>，是智能体在动作选择过程中想要最大化的目标</p>
<p><img src="https://cdn.niceasiv.cn/202303141055151.webp" alt="强化学习交互图"></p>
<ul>
<li><strong>“奖励 &amp; 收益” 其实是智能体目标的一种形式化、数值化的表征</strong>。可以把这种想法非正式地表述为 “收益假设”</li>
<li>收益是通过奖励信号计算的，而奖励函数是我们提供的，<strong>奖励函数起到了人与算法沟通的桥梁作用</strong></li>
<li>需要注意的是，智能体只会学习如何最大化收益，如果想让它完成某些指定任务，就<strong>必须保证我们设计的奖励函数可以使得智能体最大化收益的同时也能实现我们的目标</strong></li>
</ul>
<h4 id="回报"><a href="#回报" class="headerlink" title="回报"></a>回报</h4><p>在一个马尔可夫奖励过程中，从第$t$时刻状态$S_t$开始，直到终止状态时，所有奖励的衰减之和称为<strong>回报</strong>（Return）</p>
<p>假设设置$\gamma &#x3D;0.5$</p>
<p><img src="https://cdn.niceasiv.cn/202303131455336.png" alt="img"><br>$$<br>G_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+\cdots&#x3D;\sum_{k&#x3D;0}^{\infty} \gamma^k R_{t+k}<br>$$<br>假设路径为1,2,3,6</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#概率转移矩阵</span></span><br><span class="line">P=[</span><br><span class="line">    [<span class="number">0.9</span>,<span class="number">0.1</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>],</span><br><span class="line">    [<span class="number">0.5</span>,<span class="number">0.0</span>,<span class="number">0.5</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>],</span><br><span class="line">    [<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.6</span>,<span class="number">0.0</span>,<span class="number">0.4</span>],</span><br><span class="line">    [<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.3</span>,<span class="number">0.7</span>],</span><br><span class="line">    [<span class="number">0.0</span>,<span class="number">0.2</span>,<span class="number">0.3</span>,<span class="number">0.5</span>,<span class="number">0.0</span>,<span class="number">0.0</span>],</span><br><span class="line">    [<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.6</span>,<span class="number">0.0</span>,<span class="number">1.0</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">P=np.array(P)</span><br><span class="line">rewards=[-<span class="number">1</span>,-<span class="number">2</span>,-<span class="number">2</span>,<span class="number">10</span>,<span class="number">1</span>,<span class="number">0</span>]</span><br><span class="line">gamma=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_chain_reward</span>(<span class="params">start_index,chain,gamma</span>):</span><br><span class="line">    reward=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(chain)):</span><br><span class="line">        reward=gamma*reward+rewards[chain[i]-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> reward</span><br><span class="line"></span><br><span class="line">chain=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">6</span>]</span><br><span class="line">start_index=<span class="number">0</span></span><br><span class="line"></span><br><span class="line">reward=compute_chain_reward(start_index,chain,gamma)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(reward)</span><br></pre></td></tr></table></figure>

<h4 id="状态价值函数"><a href="#状态价值函数" class="headerlink" title="状态价值函数"></a>状态价值函数</h4><p>一个状态的期望回报(即从这个状态出发的未来累积奖励的期望)-&gt;价值</p>
<p>即其状态价值函数$V_{\pi}(s)$就等于转移到每个通路上的概率（由策略决定）乘以每条路上得到的回报即</p>
<p>$V(s)&#x3D;\mathbb{E}\left[G_t \mid S_t&#x3D;s\right]$ </p>
<p>展开为:<br>$$<br>\begin{align}<br>V(s) &amp;&#x3D;\mathbb{E}[G_t \mid S_t&#x3D;s] \<br>     &amp;&#x3D;\mathbb{E}[R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+…\mid S_t&#x3D;s]\<br>     &amp;&#x3D;\mathbb{E}[R_t+\gamma(R_{t+1}+\gamma R_{t+2})+… \mid S_t&#x3D;s]\<br>     &amp;&#x3D;\mathbb{E}[R_t+\gamma G_{t+1}\mid S_t&#x3D;s]\<br>     &amp;&#x3D;\mathbb{E}[R_t+\gamma V(S_{t+1})\mid S_t&#x3D;s]\<br>\end{align}<br>$$<br>一方面,即时奖励的期望正是奖励函数的输出，即：</p>
<p> $\mathbb{E}\left[R_t \mid S_t&#x3D;s\right]&#x3D;r(s)$</p>
<p>另一方面，等式中剩余部分 $\mathbb{E}\left[\gamma V\left(S_{t+1}\right) \mid S_t&#x3D;s\right]$ 可以根据从状态$s$出发的转移概率得到，即可以得到<br>$$<br>V(s)&#x3D;r(s)+\gamma\sum_{s^{\prime} \in S }{p(s^{\prime} \mid s)V(s^\prime)}<br>$$<br>上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。</p>
<p><img src="https://cdn.niceasiv.cn/202303141138303.jpeg" alt="img"></p>
<p>上式就是马尔可夫奖励过程中非常有名的贝尔曼方程 (Bellman equation)，对每一个状态都成立。</p>
<p>若一个马尔可夫奖励过程一共有 $n$ 个状 态，即 $\mathcal{S}&#x3D;\left{s_1, s_2, \ldots, s_n\right}$ ，我们将所有状态的价值表示成一个列向量 $\mathcal{V}&#x3D;\left[V\left(s_1\right), V\left(s_2\right), \ldots, V\left(s_n\right)\right]^T$ ，同理，将奖励函数写成一个列向量 $\mathcal{R}&#x3D;\left[r\left(s_1\right), r\left(s_2\right), \ldots, r\left(s_n\right)\right]^T$ 。于是我们可以将贝尔曼方程写成矩阵的形式:<br>$$<br>\begin{gathered}<br>\mathcal{V}&#x3D;\mathcal{R}+\gamma \mathcal{P} \mathcal{V} \<br>{\left[\begin{array}{c}<br>V\left(s_1\right) \<br>V\left(s_2\right) \<br>\ldots \<br>V\left(s_n\right)<br>\end{array}\right]&#x3D;\left[\begin{array}{c}<br>r\left(s_1\right) \<br>r\left(s_2\right) \<br>\ldots \<br>r\left(s_n\right)<br>\end{array}\right]+\gamma\left[\begin{array}{cccc}<br>P\left(s_1 \mid s_1\right) &amp; p\left(s_2 \mid s_1\right) &amp; \ldots &amp; P\left(s_n \mid s_1\right) \<br>P\left(s_1 \mid s_2\right) &amp; P\left(s_2 \mid s_2\right) &amp; \ldots &amp; P\left(s_n \mid s_2\right) \<br>\ldots &amp; &amp; &amp; \<br>P\left(s_1 \mid s_n\right) &amp; P\left(s_2 \mid s_n\right) &amp; \ldots &amp; P\left(s_n \mid s_n\right)<br>\end{array}\right]\left[\begin{array}{c}<br>V\left(s_1\right) \<br>V\left(s_2\right) \<br>\ldots \<br>V\left(s_n\right)<br>\end{array}\right]}<br>\end{gathered}<br>$$<br>我们可以直接根据矩阵运算求解，得到以下解析解:<br>$$<br>\begin{aligned}<br>\mathcal{V} &amp; &#x3D;\mathcal{R}+\gamma \mathcal{P} \mathcal{V} \<br>(I-\gamma \mathcal{P}) \mathcal{V} &amp; &#x3D;\mathcal{R} \<br>\mathcal{V} &amp; &#x3D;(I-\gamma \mathcal{P})^{-1} \mathcal{R}<br>\end{aligned}<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_state_value</span>(<span class="params">P,rewards,gamma,states_num</span>):</span><br><span class="line">    rewards=np.array(rewards).reshape((-<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">    value=np.dot(np.linalg.inv(np.eye(states_num)-gamma*P),rewards)</span><br><span class="line">    <span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">state_value=compute_state_value(P,rewards,gamma,<span class="number">6</span>)</span><br><span class="line"><span class="built_in">print</span>(state_value)</span><br></pre></td></tr></table></figure>

<h4 id="动作价值函数"><a href="#动作价值函数" class="headerlink" title="动作价值函数"></a>动作价值函数</h4><p>我们用 $Q^\pi(s, a)$ 表示在 MDP 遵循策略 $\pi$ 时，对当前状态 $s$ 执行动作 $a$ 得到的期望回报:<br>$$<br>Q^\pi(s, a)&#x3D;\mathbb{E}<em>\pi\left[G_t \mid S_t&#x3D;s, A_t&#x3D;a\right]<br>$$<br>状态价值函数和动作价值函数之间的关系：在使用策略 $\pi$ 中，状态 $s$ 的价值等于在该状态下基于策略 $\pi$ 采取所有动作的概率与相应的价值相乘 再求和的结果:<br>$$<br>V^\pi(s)&#x3D;\sum</em>{a \in A} \pi(a \mid s) Q^\pi(s, a)<br>$$<br>使用策略 $\pi$ 时，状态 $s$ 下采取动作 $a$ 的价值等于即时奖励加上经过衰减后的所有可能的下一个状态的状态转移概率与相应的价值的乘积:<br>$$<br>Q^\pi(s, a)&#x3D;r(s, a)+\gamma \sum_{s^{\prime} \in S} P\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)<br>$$</p>
<h4 id="贝尔曼期望方程"><a href="#贝尔曼期望方程" class="headerlink" title="贝尔曼期望方程"></a>贝尔曼期望方程</h4><p>$$<br>\begin{aligned}<br>V^\pi(s) &amp; &#x3D;\mathbb{E}<em>\pi\left[R_t+\gamma V^\pi\left(S</em>{t+1}\right) \mid S_t&#x3D;s\right] \<br>&amp; &#x3D;\sum_{a \in A} \pi(a \mid s)\left(r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right) \<br>\end{aligned}<br>$$</p>
<p>$$<br>\begin{aligned}<br>Q^\pi(s, a) &amp; &#x3D;\mathbb{E}<em>\pi\left[R_t+\gamma Q^\pi\left(S</em>{t+1}, A_{t+1}\right) \mid S_t&#x3D;s, A_t&#x3D;a\right] \<br>&amp; &#x3D;r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) \sum_{a^{\prime} \in A} \pi\left(a^{\prime} \mid s^{\prime}\right) Q^\pi\left(s^{\prime}, a^{\prime}\right)<br>\end{aligned}<br>$$</p>
<h4 id="MDP"><a href="#MDP" class="headerlink" title="MDP"></a>MDP</h4><p>$\langle \mathcal{S},\mathcal{A},P,r,\gamma \rangle$</p>
<ul>
<li><p>$\mathcal{S}$是状态集合</p>
</li>
<li><p>$\mathcal{A}$ 是动作的集合；</p>
</li>
<li><p>$\gamma$ 是折扣因子;</p>
</li>
<li><p>$r(s, a)$ 是奖励函数，此时奖励可以同时取决于状态 $s$ 和动作 $a$ ，在奖励函数 只取决于状态 $s$ 时，则退化为 $r(s)$ ；</p>
</li>
<li><p>$P\left(s^{\prime} \mid s, a\right)$ 是状态转移函数，表示在状态 $s$ 执行动作 $a$ 之后到达状态 $s^{\prime}$ 的概 率。</p>
</li>
</ul>
<p><img src="https://cdn.niceasiv.cn/202303141055151.webp" alt="强化学习交互图"></p>
<p><img src="https://cdn.niceasiv.cn/202303151058328.png" alt="img"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#mdp过程</span></span><br><span class="line">state=[<span class="string">&#x27;s1&#x27;</span>,<span class="string">&#x27;s2&#x27;</span>,<span class="string">&#x27;s3&#x27;</span>,<span class="string">&#x27;s4&#x27;</span>,<span class="string">&#x27;s5&#x27;</span>,<span class="string">&#x27;s6&#x27;</span>]</span><br><span class="line">action=[<span class="string">&#x27;s1-&gt;s1&#x27;</span>,<span class="string">&#x27;s1-&gt;s2&#x27;</span>,<span class="string">&#x27;s2-&gt;s1&#x27;</span>,<span class="string">&#x27;s2-&gt;s3&#x27;</span>,<span class="string">&#x27;s3-&gt;s4&#x27;</span>,<span class="string">&#x27;s3-&gt;s5&#x27;</span>,<span class="string">&#x27;s4-&gt;s5&#x27;</span>,<span class="string">&#x27;s4-&gt;s3&#x27;</span>,<span class="string">&#x27;s4-&gt;s4&#x27;</span>,<span class="string">&#x27;s4-&gt;s2&#x27;</span>]</span><br><span class="line"><span class="comment">#状态转移概率</span></span><br><span class="line">P=&#123;</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s1&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s2&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s1&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s3&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s4&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s5&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s5&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s3&#x27;</span>:<span class="number">0.2</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s4&#x27;</span>:<span class="number">0.4</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s2&#x27;</span>:<span class="number">0.2</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#奖励函数</span></span><br><span class="line">rewards=&#123;</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s1&#x27;</span>:-<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s2&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s1&#x27;</span>:-<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s3&#x27;</span>:-<span class="number">2.0</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s4&#x27;</span>:-<span class="number">2.0</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s5&#x27;</span>:<span class="number">0</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s5&#x27;</span>:<span class="number">10</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s3&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s4&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s2&#x27;</span>:<span class="number">1.0</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">gamma=<span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">MDP=(state,action,P,rewards,gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment">#策略1 随机策略</span></span><br><span class="line">Pi_1=&#123;</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s1&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s2&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s1&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s3&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s4&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s5&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s5&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s4-s*&#x27;</span>:<span class="number">0.5</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">#策略2</span></span><br><span class="line">Pi_2=&#123;</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s1&#x27;</span>:<span class="number">0.6</span>,</span><br><span class="line">    <span class="string">&#x27;s1-&gt;s2&#x27;</span>:<span class="number">0.4</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s1&#x27;</span>:<span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&#x27;s2-&gt;s3&#x27;</span>:<span class="number">0.7</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s4&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s3-&gt;s5&#x27;</span>:<span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s5&#x27;</span>:<span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&#x27;s4-&gt;s*&#x27;</span>:<span class="number">0.9</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如何计算MDP下，一个策略$\pi$的状态价值函数</p>
<p>MDP-&gt;MRP</p>
<p>奖励函数<br>$$<br>r^\prime(s)&#x3D;\sum_{a \in \mathcal{A}}\pi(a \mid s)r(s,a)<br>$$</p>
<p>状态转移<br>$$<br>P^\prime(s^\prime|s)&#x3D;\sum_{a \in \mathcal{A}}{\pi(a \mid s)P(s^\prime \mid s,a)}<br>$$</p>
<h4 id="Monte-Carlo"><a href="#Monte-Carlo" class="headerlink" title="Monte Carlo"></a>Monte Carlo</h4><p>$$<br>V^\pi(s)&#x3D;\mathbb{E}<em>\pi\left[G_t \mid S_t&#x3D;s\right] \approx \frac{1}{N} \sum</em>{i&#x3D;1}^N G_t^{(i)}<br>$$</p>
<p><code>步骤</code>:</p>
<ol>
<li>使用策略 $\pi$ 采样若干条序列:<br>$$<br>s_0^{(i)} \stackrel{a_0^{(i)}}{\longrightarrow} r_0^{(i)}, s_1^{(i)} \stackrel{a_1^{(i)}}{\longrightarrow} r_1^{(i)}, s_2^{(i)} \stackrel{a_2^{(i)}}{\longrightarrow} \cdots \stackrel{a_{T-1}^{(i)}}{\longrightarrow} r_{T-1}^{(i)}, s_T^{(i)}<br>$$</li>
<li>对每一条序列中的每一时间步 $t$ 的状态 $s$ 进行以下操作:</li>
</ol>
<ul>
<li>更新状态 $s$ 的计数器 $N(s) \leftarrow N(s)+1$;</li>
<li>更新状态 $s$ 的总回报 $M(s) \leftarrow M(s)+G_t$ ；</li>
</ul>
<ol start="3">
<li>每一个状态的价值被估计为回报的平均值 $V(s)&#x3D;M(s) &#x2F; N(s)$ 。<br>根据大数定律，当 $N(s) \rightarrow \infty$ ，有 $V(s) \rightarrow V^\pi(s)$ 。计算回报的期望时，除了可以把所有的回报加起来除以次数，还有一种增量更新的方法。对于每个状态 $s$ 和对应回报$G$ ，进行如下计算:</li>
</ol>
<ul>
<li>$N(s) \leftarrow N(s)+1$</li>
<li>$V(s) \leftarrow V(s)+\frac{1}{N(s)}(G-V(S))$</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">S = [<span class="string">&quot;s1&quot;</span>, <span class="string">&quot;s2&quot;</span>, <span class="string">&quot;s3&quot;</span>, <span class="string">&quot;s4&quot;</span>, <span class="string">&quot;s5&quot;</span>]  <span class="comment"># 状态集合</span></span><br><span class="line">A = [<span class="string">&quot;保持s1&quot;</span>, <span class="string">&quot;前往s1&quot;</span>, <span class="string">&quot;前往s2&quot;</span>, <span class="string">&quot;前往s3&quot;</span>, <span class="string">&quot;前往s4&quot;</span>, <span class="string">&quot;前往s5&quot;</span>, <span class="string">&quot;概率前往&quot;</span>]  <span class="comment"># 动作集合</span></span><br><span class="line"><span class="comment"># 状态转移函数</span></span><br><span class="line">P = &#123;</span><br><span class="line">    <span class="string">&quot;s1-保持s1-s1&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s1-前往s2-s2&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s1-s1&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s3-s3&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s4-s4&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s5-s5&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s4-前往s5-s5&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往-s2&quot;</span>: <span class="number">0.2</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往-s3&quot;</span>: <span class="number">0.4</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往-s4&quot;</span>: <span class="number">0.4</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 奖励函数</span></span><br><span class="line">R = &#123;</span><br><span class="line">    <span class="string">&quot;s1-保持s1&quot;</span>: -<span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;s1-前往s2&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s1&quot;</span>: -<span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s3&quot;</span>: -<span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s4&quot;</span>: -<span class="number">2</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s5&quot;</span>: <span class="number">0</span>,</span><br><span class="line">    <span class="string">&quot;s4-前往s5&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往&quot;</span>: <span class="number">1</span>,</span><br><span class="line">&#125;</span><br><span class="line">gamma = <span class="number">0.5</span>  <span class="comment"># 折扣因子</span></span><br><span class="line">MDP = (S, A, P, R, gamma)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略1,随机策略</span></span><br><span class="line">Pi_1 = &#123;</span><br><span class="line">    <span class="string">&quot;s1-保持s1&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s1-前往s2&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s1&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s3&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s4&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s5&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s4-前往s5&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 策略2</span></span><br><span class="line">Pi_2 = &#123;</span><br><span class="line">    <span class="string">&quot;s1-保持s1&quot;</span>: <span class="number">0.6</span>,</span><br><span class="line">    <span class="string">&quot;s1-前往s2&quot;</span>: <span class="number">0.4</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s1&quot;</span>: <span class="number">0.3</span>,</span><br><span class="line">    <span class="string">&quot;s2-前往s3&quot;</span>: <span class="number">0.7</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s4&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s3-前往s5&quot;</span>: <span class="number">0.5</span>,</span><br><span class="line">    <span class="string">&quot;s4-前往s5&quot;</span>: <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">&quot;s4-概率前往&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">join</span>(<span class="params">str1, str2</span>):</span><br><span class="line">    <span class="keyword">return</span> str1 + <span class="string">&#x27;-&#x27;</span> + str2</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">MDP, Pi, timestep_max, number</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27; 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number &#x27;&#x27;&#x27;</span></span><br><span class="line">    S, A, P, R, gamma = MDP</span><br><span class="line">    episodes = []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(number):</span><br><span class="line">        episode = []</span><br><span class="line">        timestep = <span class="number">0</span></span><br><span class="line">        s = S[np.random.randint(<span class="number">4</span>)]  <span class="comment"># 随机选择一个除s5以外的状态s作为起点</span></span><br><span class="line">        <span class="comment"># 当前状态为终止状态或者时间步太长时,一次采样结束</span></span><br><span class="line">        <span class="keyword">while</span> s != <span class="string">&quot;s5&quot;</span> <span class="keyword">and</span> timestep &lt;= timestep_max:</span><br><span class="line">            timestep += <span class="number">1</span></span><br><span class="line">            rand, temp = np.random.rand(), <span class="number">0</span></span><br><span class="line">            <span class="comment"># 在状态s下根据策略选择动作</span></span><br><span class="line">            <span class="keyword">for</span> a_opt <span class="keyword">in</span> A:</span><br><span class="line">                temp += Pi.get(join(s, a_opt), <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> temp &gt; rand:</span><br><span class="line">                    a = a_opt</span><br><span class="line">                    r = R.get(join(s, a), <span class="number">0</span>)</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            rand, temp = np.random.rand(), <span class="number">0</span></span><br><span class="line">            <span class="comment"># 根据状态转移概率得到下一个状态s_next</span></span><br><span class="line">            <span class="keyword">for</span> s_opt <span class="keyword">in</span> S:</span><br><span class="line">                temp += P.get(join(join(s, a), s_opt), <span class="number">0</span>)</span><br><span class="line">                <span class="keyword">if</span> temp &gt; rand:</span><br><span class="line">                    s_next = s_opt</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">            episode.append((s, a, r, s_next))  <span class="comment"># 把（s,a,r,s_next）元组放入序列中</span></span><br><span class="line">            s = s_next  <span class="comment"># s_next变成当前状态,开始接下来的循环</span></span><br><span class="line">        episodes.append(episode)</span><br><span class="line">    <span class="keyword">return</span> episodes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># # 采样5次,每个序列最长不超过20步</span></span><br><span class="line"><span class="comment"># episodes = sample(MDP, Pi_1, 20, 5)#策略为Pi_1</span></span><br><span class="line"><span class="comment"># 对所有采样序列计算所有状态的价值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">MC</span>(<span class="params">episodes, V, N, gamma</span>):</span><br><span class="line">    <span class="keyword">for</span> episode <span class="keyword">in</span> episodes:</span><br><span class="line">        G = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(episode) - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):  <span class="comment">#一个序列从后往前计算</span></span><br><span class="line">            (s, a, r, s_next) = episode[i]</span><br><span class="line">            G = r + gamma * G</span><br><span class="line">            N[s] = N[s] + <span class="number">1</span></span><br><span class="line">            V[s] = V[s] + (G - V[s]) / N[s]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">timestep_max = <span class="number">20</span></span><br><span class="line"><span class="comment"># 采样1000次,可以自行修改</span></span><br><span class="line">episodes = sample(MDP, Pi_1, timestep_max, <span class="number">1000</span>)</span><br><span class="line">gamma = <span class="number">0.5</span></span><br><span class="line">V = &#123;<span class="string">&quot;s1&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s2&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s3&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s4&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s5&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">N = &#123;<span class="string">&quot;s1&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s2&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s3&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s4&quot;</span>: <span class="number">0</span>, <span class="string">&quot;s5&quot;</span>: <span class="number">0</span>&#125;</span><br><span class="line">MC(episodes, V, N, gamma)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;使用蒙特卡洛方法计算MDP的状态价值为\n&quot;</span>, V)</span><br></pre></td></tr></table></figure>

<h4 id="最优策略"><a href="#最优策略" class="headerlink" title="最优策略"></a>最优策略</h4><p>对于任意的状态 $s$ 都有 $V^\pi(s) \geq V^{\pi^{\prime}}(s)$ ，记 $\pi&gt;\pi^{\prime}$ 。</p>
<p>于是在有限状态 和动作集合的 MDP 中，至少存在一个策略比其他所有策略都好或者 至少存在一个策略不差于其他所有策略，这个策略就是<code>最优策略 (optimal policy)</code> 。最优策略可能有很多个，我们都将其表示为 $\pi^*(s)$<br>最优策略都有相同的状态价值函数，我们称之为最优状态价值函数， 表示为:<br>$$<br>V^*(s)&#x3D;\max <em>\pi V^\pi(s), \quad \forall s \in \mathcal{S}<br>$$<br>同理，我们定义最优动作价值函数:<br>$$<br>Q^*(s, a)&#x3D;\max <em>\pi Q^\pi(s, a), \quad \forall s \in \mathcal{S}, a \in \mathcal{A}<br>$$<br>最优状态价值函数和最优动作价值函数之间的关系：<br>$$<br>Q^*(s, a)&#x3D;r(s,a)+\gamma\sum</em>{s^\prime \in S}P(s^\prime \mid s,a)V^*(s^\prime)<br>$$<br>最优状态价值是选择此时使<code>最优动作价值</code>最大的那一个动作时的状态价值<br>$$<br>V^*(s)&#x3D;\max</em>{a \in \mathcal{A}}Q^*(s,a)<br>$$</p>
<h4 id="Bellman最优方程"><a href="#Bellman最优方程" class="headerlink" title="Bellman最优方程"></a>Bellman最优方程</h4><p>根据 $V^*(s)$ 和 $Q^*(s, a)$ 的关系，我们可以得到贝尔曼最优方程<br>(Bellman optimality equation) :<br>$$<br>\begin{gathered}<br>V^*(s)&#x3D;\max <em>{a \in \mathcal{A}}\left{r(s, a)+\gamma \sum</em>{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) V^<em>\left(s^{\prime}\right)\right} \<br>Q^</em>(s, a)&#x3D;r(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) \max _{a^{\prime} \in \mathcal{A}} Q^*\left(s^{\prime}, a^{\prime}\right)<br>\end{gathered}<br>$$</p>
<h3 id="DP优化"><a href="#DP优化" class="headerlink" title="DP优化"></a>DP优化</h3><p>基于动态规划的强化学习算法主要有两种一是<code>策略迭代</code>,二是<code>价值迭代</code></p>
<p>策略迭代:策略评估(使用贝尔曼期望方程-&gt;策略的状态价值函数)+策略提升</p>
<p><code>策略迭代算法</code><br>$$<br>\begin{aligned}<br>V^\pi(s) &amp; &#x3D;\mathbb{E}<em>\pi\left[R_t+\gamma V^\pi\left(S</em>{t+1}\right) \mid S_t&#x3D;s\right] \<br>&amp; &#x3D;\sum_{a \in A} \pi(a \mid s)\left(r(s, a)+\gamma \sum_{s^{\prime} \in S} p\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right) \<br>\end{aligned}<br>$$</p>
<h2 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h2><h2 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h2><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/1765772c8444">https://www.jianshu.com/p/1765772c8444</a></p>

    </div>

    
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by Asiv, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
        </div>
        
        <p class="tags">
            
            <i class="icon"></i>
            <a href="/tags/Reinforcement-learning/" class="tag">#Reinforcement learning</a><a href="/tags/Machine-Learning/" class="tag">#Machine Learning</a>
        </p>
        
    </div>
    

    <div class="container post-prev-next">
        
        <a href="/2023/03/10/pythonNote/" class="next">
            <div>
                <div class="text">
                    <p class="label">Next</p>
                    <h3 class="title">Python复习</h3>
                </div>
            </div>
        </a>
        
        
        <a href="/2023/03/04/Policy_Distillation/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">策略蒸馏</>
                </div>
            </div>
        </a>
        
    </div>

    
        
        
    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
            <div class="group">
                <h2 class="title">Blog</h2>
                
                <a href="/" class="item">Blog</a>
                
                <a href="/archives" class="item">Archives</a>
                
                <a href="/search" class="item">Search</a>
                
                <a href="/friends" class="item">Friends</a>
                
            </div>
            
            <div class="group">
                <h2 class="title">Me</h2>
                
                <a target="_blank" rel="noopener" href="https://github.com/NiceAsiv" class="item">GitHub</a>
                
                <a href="mailto:asiwuhe@hotmail.com" class="item">mail</a>
                
                <a href="/about" class="item">About</a>
                
            </div>
            
        </div>
        <span>&copy; 2023 Asiv<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>