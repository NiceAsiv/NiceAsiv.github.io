<!DOCTYPE html>
<html lang="zh-CN">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>策略蒸馏 - Asiv&#39;s Blog</title>
  
    <link rel="shortcut icon" href="/img/avatar.jpg">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="策略蒸馏 - Asiv&#39;s Blog" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://niceasiv.cn/2023/03/04/Policy_Distillation/index.html" />
  
  <meta property="og:image" content="/favicon.png" />
  
  <meta property="og:article:published_time" content="2023-03-04T08:06:09.000Z" />
  
  <meta property="og:article:author" content="Asiv" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-rainbow-banner="true"
        data-rainbow-banner-shown="auto"
        data-rainbow-banner-month="3"
        data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="3"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Asiv&#39;s Blog</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/friends">Friends</a>
            
            
            
            <a class="nav-item" href="/essay">Essay</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/NiceAsiv" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        
        <div class="categories text-uppercase">
        
            <a href="/categories/Machine-Learning/">Machine Learning</a>
        
        </div>
        

        
        <div class="date" id="date">
            <span>March</span>
            <span>4,</span>
            <span>2023</span>
        </div>
        

        <h2 class="title">策略蒸馏</h2>
    </div>

    <div class="divider"></div>

    <div class="content">
        <p>为了理解蒸馏的含义，我们先以DQN为例</p>
<p>DQN（Deep Q-Network）是一种强化学习算法，它通过深度神经网络来近似Q函数，从而实现智能体的行为决策。下面我们将介绍一个针对DQN网络的案例。</p>
<p>假设我们有一个小型的迷宫环境，智能体的任务是通过左、右、上、下四个动作来找到宝藏并获得最高的奖励。我们可以使用DQN算法来训练智能体。</p>
<p>首先，我们需要定义状态空间、动作空间、奖励函数以及转移函数。在这个案例中，状态空间是迷宫中每个位置的坐标，动作空间是四个方向，奖励函数是在找到宝藏时获得的奖励，转移函数是在智能体执行一个动作后转移到下一个状态的过程。</p>
<p>然后，我们可以使用深度神经网络来近似Q函数。在这个案例中，我们可以使用一个简单的全连接神经网络，它的输入是当前状态，输出是四个动作的Q值。我们使用均方误差损失函数来度量Q值的预测误差，并使用梯度下降算法来优化神经网络参数。</p>
<p>接下来，我们可以使用经验回放机制来训练DQN网络。经验回放是一种存储智能体的经验，并从中随机抽样的机制。这样可以使得训练数据更加丰富、稳定，并且可以避免连续的训练样本之间的相关性。在每次训练中，我们随机从经验池中选择一个批次的样本，然后用它来更新神经网络参数。</p>
<p>最后，我们可以通过迭代训练来提高DQN网络的性能。我们通过不断地与环境交互、收集经验、更新神经网络参数来提高DQN网络的预测性能。在训练的过程中，我们可以使用ε-greedy策略来平衡探索和利用，这样可以使得智能体在探索新状态和利用已有知识之间找到一个平衡点。</p>
<p>通过以上的训练过程，我们可以得到一个训练好的DQN网络，它可以在迷宫环境中通过左、右、上、下四个动作来找到宝藏并获得最高的奖励。</p>
<p>下面是一个基于DQN算法的伪代码：</p>
<p>初始化神经网络 Q(s,a;θ) 的参数 θ 初始化目标网络 Q’(s,a;θ^-) 的参数 θ^-</p>
<p>初始化经验池 D</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">for episode = 1 to M do: 初始化状态 s </span><br><span class="line">for t = 1 to T do:</span><br><span class="line">    选择动作 a ：</span><br><span class="line">        如果随机数小于 ε，则选择一个随机动作</span><br><span class="line">        否则，选择 a = argmax Q(s,a;θ)</span><br><span class="line">    </span><br><span class="line">    执行动作 a 并观察下一个状态 s&#x27; 和奖励 r</span><br><span class="line">    </span><br><span class="line">    存储经验 (s, a, r, s&#x27;) 到经验池 D</span><br><span class="line">    </span><br><span class="line">    从经验池 D 中随机抽取一个批次的经验 (s_i, a_i, r_i, s&#x27;_i)</span><br><span class="line">    </span><br><span class="line">    对于批次中的每个样本 (s_i, a_i, r_i, s&#x27;_i) ，计算目标 Q 值：</span><br><span class="line">        如果 s&#x27;_i 是终止状态，则 Q_target = r_i</span><br><span class="line">        否则，Q_target = r_i + γ max Q&#x27;(s&#x27;_i, a&#x27;;θ^-)</span><br><span class="line">    </span><br><span class="line">    使用 Q(s_i, a_i;θ) 和 Q_target 之间的均方误差来更新神经网络 Q(s,a;θ) 的参数 θ</span><br><span class="line">    </span><br><span class="line">    如果 t mod C == 0，那么将目标网络的参数 θ^- 更新为 Q(s,a;θ) 的参数 θ</span><br><span class="line">    </span><br><span class="line">    将状态更新为 s&#x27;</span><br><span class="line"></span><br><span class="line">减小 ε</span><br><span class="line">保存神经网络 Q(s,a;θ) 的参数 θ</span><br></pre></td></tr></table></figure>

<p>其中，M 是迭代训练的次数，T 是每个 episode 的最大步数，ε 是 ε-greedy 策略中的探索率，γ 是折扣因子，C 是更新目标网络的间隔，D 是经验池，Q 和 Q’ 分别是当前网络和目标网络。</p>

    </div>

    
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by Asiv, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
        </div>
        
        <p class="tags">
            
            <i class="icon"></i>
            <a href="/tags/Distillation/" class="tag">#Distillation</a><a href="/tags/Reinforcement-learning/" class="tag">#Reinforcement learning</a>
        </p>
        
    </div>
    

    <div class="container post-prev-next">
        
        <a href="/2023/03/04/reinforment_learning/" class="next">
            <div>
                <div class="text">
                    <p class="label">Next</p>
                    <h3 class="title">强化学习笔记</h3>
                </div>
            </div>
        </a>
        
        
        <a href="/2022/11/28/PE_DLL/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">通过修改 PE 装载 DLL 实验</>
                </div>
            </div>
        </a>
        
    </div>

    
        
        
    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
            <div class="group">
                <h2 class="title">Blog</h2>
                
                <a href="/" class="item">Blog</a>
                
                <a href="/archives" class="item">Archives</a>
                
                <a href="/search" class="item">Search</a>
                
                <a href="/friends" class="item">Friends</a>
                
            </div>
            
            <div class="group">
                <h2 class="title">Me</h2>
                
                <a target="_blank" rel="noopener" href="https://github.com/NiceAsiv" class="item">GitHub</a>
                
                <a href="mailto:asiwuhe@hotmail.com" class="item">mail</a>
                
                <a href="/about" class="item">About</a>
                
            </div>
            
        </div>
        <span>&copy; 2023 Asiv<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>